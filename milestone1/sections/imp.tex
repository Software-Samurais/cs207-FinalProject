\textit{Reverse Mode} and \textit{Forward Mode} will be implemented separately with different data structures but shared basic functions like \mintinline{Python}{Add}, \mintinline{Python}{Mul} in the file \textit{basic\_funcs.py}.

\subsection{Reverse Mode}
\paragraph{Data Structure:}The first step is to parse the input equation into a computational graph for the forward pass. Every leaf variables~(aside from constants) is an instance of \mintinline{Python}{Node}. Computation between leaf variables will be reloaded and return a new node. By doing this, we can parse the input equation into a computational graph automatically. 
\paragraph{Classes and attributes:}Every node in the computation graph is an instance of class \mintinline{Python}{Node} which includes \mintinline{Python}{self.inputs}, the input nodes of current node.
\newline\mintinline{Python}{compute(self, node, input_vals)} and \mintinline{Python}{gradient(self, node, output_grad)}. 
\paragraph{Methods:}Since the order of gradient computing is crucial for \textit{Reverse Mode}, we will use a simple algorithm which does a post-order Depth-First-Search~(DFS) traversal on the given nodes in the computational graph, going backwards based on input edges. With post-order DFS, a node will be added to the ordering after all its predecessors are traversed and we can get a topological sort finally. Then, we can reverse the topological sort list of nodes and start from the output nodes to compute gradient. Pseudo codes are shown as follows,
\begin{minted}{python}
def gradient(out):
    node_to_grad = {}
    nodes = get_reverse_topo_order(out)
    for node in nodes:
        # get gradient of current node
        grad = sum(partial_adjoints from output_edges)  
        # get partial gradient of input nodes of current node
        input_grads = [node.gradient(i, grad) for i in node.inputs]
        add input_grads to node_to_grad
    return node_to_grad
\end{minted}
\paragraph{External dependencies:} Numpy is used to support operations between matrix like \mintinline{python}{MatMul}. 
\paragraph{Elementary Functions:} We will implement them inside the class of \mintinline{Python}{Node}. For example, for the method of \mintinline{Python}{__add__}, we can implement it as follows,
\begin{minted}{python}
class Node(object):
    def __add__(self, other):
        if isinstance(other, Node):
            new_node = add_op(self, other)
        else:
            new_node = add_byconst_op(self, other)
        return new_node
     # Allow left-hand-side add and multiply.
    __radd__ = __add__
\end{minted}

\subsection{Forward Mode}
\textit{Forward Mode} is easier than \textit{Reverse Mode} to implement with different data structure. 
\paragraph{Data Structure:}We will use \mintinline{Python}{STACK} as the main data structure. Basically, each equation can be computed sequentially with Reverse Polish notation~(RPN). For each node, we have to carry derivatives along with evaluation flow. In addition, we have to save the whole graph because we do not know whether we will use them again. 
\paragraph{Classes and attributes:}We will use the class of \mintinline{Python}{Node} again. However, we don't have to maintain the whole graph this time.
Following the order of RPN, a new node will be added with two main attributes, \mintinline{Python}{value} and \mintinline{Python}{derivation}. 

It is worth mentioning that \mintinline{Python}{derivation} of each node is a list with the same length. Specifically, the first element of the list is the derivation of the first leaf variable with respect to the current node. In addition, \mintinline{Python}{derivation} is computed based on the type of gradient and the input gradient. 


