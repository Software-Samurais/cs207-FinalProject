The increasing importance of computational models in science and business alongside the slowing pace of advances in computing hardware has increased the need for efficient and accurate evaluations of derivatives. Many important applications such as simulation, optimization, and neural networks rely on repeated differentiation of complex functions. 

Before the advent of automatic differentiation (AD) the primary method for derivative evaluation was the method of finite differences (FD), where the function to be evaluated is effectively treated as black box oracle.\footnote{See the \emph{Background} section for a more detailed introduction} As the FD method is effectively sampling, the granularity (i.e. step size) of the algorithm can introduce error effects if it is either too large or too small, but even at the perfect medium, $f'(x)$ evaluations cannot reach machine precision. The alternative approach, fully symbolic differentiation (SD), is cumbersome and inefficient in many cases. In the case of a complex computer program, the size of the symbolic expression could grow to outrageous size and cause significant inefficiency.  

The approach of algorithmic differentiation seeks to find the best of both worlds, with machine precision and easy evaluation. This is done by repeated evaluation of the chain rule at a point stored in a table called the computational trace. Thus rather than storing to full symbolic expression, an AD code only needs to apply the chain rule to a specific evaluation, representable by a single variable. This approach allows us to achieve the accuracy of symbolic approaches while drastically reducing the cost of evaluation. 

Within the umbrella of automatic differentiation, we seek to implement the forward mode which evaluates the intermediate results directly in an inside out manner. Other approaches such as reverse mode also have specific advantages especially in the fields of machine learning and artificial intelligence-- or in any context in which the number of inputs dominates the number of outputs.  

The method of automatic differentiation, sometimes also referred to as 
algorithmic differentiation, addresses the weaknesses of the finite 
difference method by providing a systematic way to calculate derivatives 
numerically to arbitrary precision. The goal of \texttt{AutoDiff} is to 
implement the forward mode of automatic differentiation, as it is a relevant
feature that even some mainstream machine learning libraries, such as 
PyTorch, lack. 

