{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
    "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS207 Project Milestone 2  - @Software-Samurais</h1> \n",
    "    \n",
    "### Group 3: \n",
    "#### Erick Ruiz, Jingyuan Liu, Kailas Amin, Simon (Xin) Dong\n",
    "\n",
    "\n",
    "\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Introduction\n",
    "\n",
    "The increasing importance of computational models in science and business alongside the slowing pace of advances in computing hardware has increased the need for efficient and accurate evaluations of derivatives. Many important applications such as simulation, opti- mization, and neural networks rely on repeated differentiation of complex functions.\n",
    "\n",
    "Before the advent of automatic differentiation (AD) the primary method for derivative evaluation was the method of finite differences (FD), where the function to be evaluated is effectively treated as black box oracle.1 As the FD method is effectively sampling, the granularity (i.e. step size) of the algorithm can introduce error effects if it is either too large or too small, but even at the perfect medium, f′(x) evaluations cannot reach machine precision. The alternative approach, fully symbolic differentiation (SD), is cumbersome and inefficient in many cases. In the case of a complex computer program, the size of the symbolic expression could grow to outrageous size and cause significant inefficiency.\n",
    "\n",
    "The approach of algorithmic differentiation seeks to find the best of both worlds, with machine precision and easy evaluation. This is done by repeated evaluation of the chain rule at a point stored in a table called the computational trace. Thus rather than storing to full symbolic expression, an AD code only needs to apply the chain rule to a specific evalua- tion, representable by a single variable. This approach allows us to achieve the accuracy of symbolic approaches while drastically reducing the cost of evaluation.\n",
    "Within the umbrella of automatic differentiation, we seek to implement the forward mode which evaluates the intermediate results directly in an inside out manner. Other approaches such as reverse mode also have specific advantages especially in the fields of machine learning and artificial intelligence– or in any context in which the number of inputs dominates the number of outputs.\n",
    "\n",
    "The method of automatic differentiation, sometimes also referred to as algorithmic differ- entiation, addresses the weaknesses of the finite difference method by providing a systematic way to calculate derivatives numerically to arbitrary precision. The goal of AutoDiff is to implement the forward mode of automatic differentiation, as it is a relevant feature that even some mainstream machine learning libraries, such as PyTorch, lack.\n",
    "\n",
    "\n",
    "# 2 Background\n",
    "Understanding the concept of a derivative is crucial to all aspiring and\n",
    "practicing scientists, engineers, and mathematicians. It is one of the\n",
    "first concepts introduced in first-year calculus courses at all\n",
    "universities. The idea is simple. Given a function, $f(x)$, how can we\n",
    "quantify the rate of change of the function due to an infinitesimal\n",
    "change, $\\Delta x$, in the argument, $x$? The answer is typically given\n",
    "in terms of the limit definition of the derivative.\n",
    "\\begin{equation}\n",
    "f'(x) = \\lim_{\\Delta x\\rightarrow 0} \\frac{f(x+\\Delta x)-f(x)}{\\Delta x}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "While equation (1) holds for any function, in practice, it is\n",
    "easier to calculate derivatives analytically according to a set of\n",
    "rules. However, obtaining an analytical expression for the derivative\n",
    "becomes exceedingly difficult if the function of interest is composed of\n",
    "many elementary functions.\\\n",
    "In any case of a derivative evaluating program, we will need to create a\n",
    "code which takes as inputs both a function and evaluation point and then\n",
    "return the derivative at that point. There are many approaches to\n",
    "perform this calculation.\\\n",
    "For example, consider the following function.\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\exp\\left[\\frac{\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}{\\cos(3x^5)}\\right]\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Calculating the first derivative would result\n",
    "in the following expression. \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    f'(x) &= \\exp\\left[\\frac{\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}{\\cos(3x^5)}\\right]\n",
    "    \\sec^2(3x^5)\\dots\\nonumber\\\\\n",
    "    &\\qquad\\times\\left\\{\n",
    "    \\frac{\\cos(3x^5)}{2\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}\n",
    "    \\left[3x^2-\\frac{1}{x}+8x\\cos(4x^2)\\right]\\dots\\right.\\nonumber\\\\\n",
    "    &\\qquad\\qquad+\n",
    "    \\left.\\vphantom{\\frac{\\cos(3x^5)}{2\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}}\n",
    "    15x^4\\sin(3x^5)\\sqrt{x^3 - \\ln x + \\sin(4x^2)}\n",
    "    \\right\\}\\end{aligned}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Although feasible, successive calculations\n",
    "become more and more complex, and in practice, the quantity to be\n",
    "differentiated may not be a function in closed-form but rather a set of\n",
    "measurements or values given as a one-dimensional vector of numbers. In\n",
    "that case, equation (1) can be approximated using the finite difference\n",
    "method, which replaces an infinitesimal change in the argument for a\n",
    "finite change. To show how this works, let us write the Taylor series\n",
    "expansion of an arbitrary function, $f(x)$, at the point $x+h$.\n",
    "\n",
    "\\begin{equation}\n",
    "f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\dots\n",
    "    \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "Keeping only terms of $\\mathcal{O}(h)$ leaves us\n",
    "with \n",
    "\n",
    "\\begin{equation}\n",
    "f(x) \\approx f(h) + hf'(x),\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "which we can rearrange to write an\n",
    "approximate expression for the derivative, $f'(x)$.\n",
    "\n",
    "\\begin{equation}\n",
    "f'(x) \\approx \\frac{f(x+h)-f(x)}{h}\n",
    "    \\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "The finite change, $h$, is called the step size, and\n",
    "equation (6) is known as the forward difference. Its geometric interpretation is\n",
    "described in Figure 1.<img src=\"./img/figure1.jpeg\" width = \"600\" alt=\"name1\" align=center />\n",
    "\n",
    "Although the finite difference method is useful and easy to implement,\n",
    "its accuracy can vary depending on the step size that is chosen. Suppose\n",
    "we wish to approximate the derivative of $f(x)=\\ln x$ using the forward\n",
    "difference method described in equation (6) using step\n",
    "sizes $h=\\{10^{-1},\\,\n",
    "10^{-7},\\,10^{-15}\\}$. This is rather unnecessary because the analytical\n",
    "derivative is just $f'(x) = 1/x$, but this example will serve to\n",
    "illustrate the drawbacks of the finite difference method. At\n",
    "$h=10^{-1}$, the numerical derivative is inaccurate because the step\n",
    "size is too large, making the calculations susceptible to truncation\n",
    "error. Conversely, at $h=10^{-15}$, the forward difference method also\n",
    "gives inaccurate results because the calculations can only be\n",
    "represented to a finite precision by the hardware in use. Hence,\n",
    "rounding error also affects the stability of the finite difference\n",
    "method.\n",
    "\n",
    "In order to evaluate the derivative via forward mode AD we first\n",
    "construct a computational graph which encodes the composition and\n",
    "dependence of sub-function evaluations. It is important to note that\n",
    "each elementary function evaluation needs to be understood at a symbolic\n",
    "level! After constructing the graph, the chain rule is simply applied\n",
    "successively to evaluations at a single point which then generates a\n",
    "table known as the computational trace. To illustrate the concept,\n",
    "consider the following example, adapted from *Evaluating derivatives:\n",
    "principles and techniques of algorithmic differentiation* by Griewank\n",
    "and Walther.\n",
    "\\begin{equation}\n",
    "f(x,y) = \\left[\\sin\\left(\\frac{x}{y}\\right) + \\frac{x}{y} - \\exp(y)\\right]\n",
    "    \\left[\\frac{x}{y} - \\exp(y)\\right]\n",
    "    \\tag{7}\n",
    "\\end{equation}\n",
    "A function of two arguments (i.e. variables $x$ and\n",
    "$y$) like the one in equation (7) can be\n",
    "evaluated at point by replacing the arguments with numerical values. The\n",
    "series of calculations needed to carry out the evaluation can be\n",
    "visualized as a computation graph, as shown in Figure 2.<img src=\"./img/figure2.png\" width = \"800\" alt=\"name\" align=center />\n",
    "The graph helps visualize the order of the\n",
    "computations, and it also establishes the dependence of successive\n",
    "calculations on previous ones.\\\n",
    "Although in many cases, the AD approach is given the entire function as\n",
    "input, this is not always the case. This necessitates the need for a\n",
    "\\\"seed\\\" value. In the basic case when the entire function is given, we\n",
    "can simply initialize the computational trace (discussed below) as 1.\n",
    "However, given the need for modularity which respects the chain rule, it\n",
    "is also possible to input a different seed, allowing for the integration\n",
    "of AD onto a sub segment of the function.\n",
    "\n",
    "\\\n",
    "The procedure for constructing such a graph is the following. Define a\n",
    "node for each of the inputs using a new variable. For this example, we\n",
    "will let $x_1 = x$ and $x_2 = y$. From there, any successive nodes may\n",
    "accept two inputs at maximum, and each node represents a new\n",
    "calculation. For example, in the computational graph shown in Figure 3, the node $x_3$ represents the calculation\n",
    "$x_1/x_2$, which later becomes an input in successive nodes. While the\n",
    "computational graph is useful for visualizing the entire computation\n",
    "procedure, the computational trace is useful for storing values as the\n",
    "computation is carried out. The computational trace for\n",
    "$f(1.5000, 0.5000)$ is given in Table 1![Table1](img/table1.png) In reference to the above, we can note that in\n",
    "this trace, the seed is 1 and thus suppressed as is often notated. In\n",
    "addition, it is important to note that depending on what implementation\n",
    "of forward mode the exact target computed can be slightly different. In\n",
    "particular, the vectorized extension of our milestone 2 code computes\n",
    "the Jacobian.\n",
    "\n",
    "Beyond the basic forward mode, there exists other implementations of AD\n",
    "in the so called \"reverse mode\\\". This approach can increase efficiency\n",
    "significantly in cases when the number of inputs is far greater than the\n",
    "number of outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 How to use\n",
    "## 3.1 <span id=\"jump\">Installation</span>\n",
    "\n",
    "The `github` url to the project is https://github.com/Software-Samurais/cs207-FinalProject.\n",
    "\n",
    "We provide two ways for our package installation: PyPI and GitHub.\n",
    "### Install from PyPI\n",
    "\n",
    "We have already uploaded our GuruDiff package to PyPI. To use the GuruDiff package, users should first run the commands provided below to install our package via pip and import it.\n",
    "\n",
    "\n",
    "- Install GuruDiff using pip:\n",
    "```shell\n",
    "pip install GuruDiff\n",
    "```\n",
    "- Import GuruDiff package to use\n",
    "```Python\n",
    "import GuruDiff.AD as AD\n",
    "```\n",
    "\n",
    "### Install from GitHub\n",
    "- Download the package from GitHub to your folder via these commands (in the terminal)\n",
    "\n",
    "```shell\n",
    "# Clone the repo\n",
    "git clone https://github.com/Software-Samurais/cs207-FinalProject.git\n",
    "\n",
    "```\n",
    "\n",
    "- Create a virtual environment and activate it (for Mac and Ubutun)\n",
    "\n",
    "```shell\n",
    "# If you don't have virtualenv, install it\n",
    "sudo easy_install virtualenv\n",
    "# Create virtual environment\n",
    "virtualenv env\n",
    "# Activate your virtual environment\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "- Install the necessary dependencies :\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- Run module tests (in the root directory):\n",
    "\n",
    "```shell\n",
    "# Run module tests if you like\n",
    "python -m pytest ./test/test_forward.py ./test/test_reverse.py\n",
    "```\n",
    "\n",
    "## 3.2 Basic Demo\n",
    "\n",
    "In this part, we will give a basic demo for a quick guide to the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Software Organization\n",
    "\n",
    "## 4.1 Directory Structure\n",
    "The package's directory will be structured as follows:\n",
    "``` py\n",
    "Ccs207-FinalProject\n",
    "    /autodiff                                           #Back-end source code\n",
    "\t    __init__.py                                     #Initialization\n",
    "\t    AD.py                                           #Forward mode: main file\n",
    "        reverse.py                                      #Reverse mode: extension feature\n",
    "\t/test                                               #Test cases\n",
    "\t\ttest_forward.py\n",
    "        test_reverse.py\n",
    "\t/docs                                               #Documentation and records\n",
    "\t\tmilestone1.pdf\n",
    "        milestone2.ipynb\n",
    "        documentation.ipynb                             #Final documentation for the package\n",
    "\t/demo                                               #Simple demos for user\n",
    "    .travis.yml                                         #Setting up Travis CI \n",
    "    setup.py                                            #Releasing package\n",
    "    requirements.txt                                    #Packages on which the program depends\n",
    "    README.md                                           #Introduction for the project\n",
    "```\n",
    "##  4.2 Basic Modules and functionality\n",
    "\n",
    "We choose to have two module published: forward mode for auto-differentiation in `AD` module and reverse mode in `reverse` module.\n",
    "\n",
    "- **AD**: This module contains our custom library for auto differentiation in forward mode. \n",
    "    - The `AD` module includes a `Var` class that contains values and derivatives, which is used to define a variable for auto differentiation. In the class, we override the operators like `__repr__`, `__neg__`, `__add__`, `__radd__`, `__sub__`, `__rsub__`, `__mul__`, `__rmul__`, `__truediv__`, `__rtruediv__`, `__pow__` and comparison operators like `__eq__` and `__ne__`.\n",
    "    - In addition, we define class-specific functions including Trig functions (sine, cosine, tangent), Inverse trig functions (arcsine, arccosine, arctangent), Exponentials with any base, Hyperbolic functions (sinh, cosh, tanh), Logistic function, Logarithms with any base, Square root. Thus the user could use our defined math function easily (as we use numpy).\n",
    "\n",
    "- **reverse**: This module contains our custom library for auto differentiation in reverse mode. It includes several class: Node, Op (including sevaral sub-class for specific operator), Executor. \n",
    "    - `Node` class defines the Node in a computation graph used in reverse mode, which contains list of input nodes, operator, constant, and node name .In the class, we also override the basic operators and comparison operators.\n",
    "    - `Op` class defines the operations performed on nodes. It could return a Node which is created by this op, real values, or a Node for the gradient computation in the reverse mode. It is the parent class for several operaor class like `SinOp`, `ExpOp` etc. Those classes inherited from `Op` define Elementary Functions for the operations that our forward mode implements.\n",
    "    - `Executor` class computes values for a given subset of nodes in a computation graph. It takes list of nodes whose values need to be computed as input.\n",
    "\n",
    "    \n",
    "## 4.3 Test Suite\n",
    "### Where do the tests live? \n",
    "\n",
    "All test files will be placed in the test folder.\n",
    "\n",
    "- `test_forward`: It includes tests for scalar or vector functions to ensure that the AD module properly calculates values of different functions and gradients with respect to scalar or vector inputs.\n",
    "\n",
    "- `test_reverse`: This is a test suite for our extention feature: reverse mode. It ensures the reverse model properly works.\n",
    "\n",
    "### How are they run? \n",
    "\n",
    "Users can run the test suite by running in the root folder:\n",
    "```shell\n",
    "python -m pytest ./test/test_forward.py #seperately test forward\n",
    "python -m pytest ./test/test_reverse.py #seperately test reverse\n",
    "python -m pytest ./test/test_forward.py ./test/test_reverse.py #test all\n",
    "```\n",
    "### How are they integrated?\n",
    "\n",
    "We would utilize Travis CI and CodeCov to make the development process more reliable and professional. \n",
    "\n",
    "- `Travis CI` is used as a distributed CI (Continuous Integration) tools to build and automate test the project.\n",
    "     \n",
    "- `CodeCov` is used for test results analysis (eg. measuring test code coverage) and visualization.    \n",
    "\n",
    "\n",
    "\n",
    "## 4.4 Software Package and Distribution:\n",
    "### Package distribution\n",
    "We will package our software using `PyPI` (Python Package Index) for release. Write and run ’setup.py’ to package the software and upload it to the distribution server, thus people in community could easily download our package by ’pip install’.\n",
    "### Version Control\n",
    "We will take Version Control into consideration according to the standard in Python Enhancement Proposal (PEP) 386. With version control, we can tell the user what changes we made and set clear boundaries for where those changes occurred.\n",
    "\n",
    "## 4.5 How to install the package\n",
    "\n",
    "User can download and install our package either using pip or manually clone from github as mentioned in [3.1 Installation](#jump).\n",
    "\n",
    "- For consumers, We suggest to follow installation procedure with pip, which is simple and easy to achieve.\n",
    "\n",
    "- For developers, they could also install from pip but they may care about the source code and the detailed implementation. If so, they could clone they repository from github and even contribute to the new version of the package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5 Implementation\n",
    "\n",
    "## 5.1 Core Data Structures\n",
    "\n",
    "In current implementation, we define our own class `AutoDiff` as the main data structure. For future work, we would use \n",
    "- **List and Numpy array** - for vectorization. Track how many independent variables have been created so far and store copies of these independent variables.\n",
    "\n",
    "- **Graph** - for reverse mode. Every leaf variables (aside from constants) is an instance of Node. A computational graph consists of multiple nodes. Each node has two main attributes, self.inputs and self.op which indicate inputs of the current node and the operation between inputs. The input list is a python list whose elements are inputs of the current node. For example, if `node_3 =  ad.Op_1(node_2, node_1)` , we have `node_3.inputs = [node_2, node_1]` and `node_3.op = ad.Op_1`. Computation between leaf variables will be reloaded and return a new node. By doing this, we can parse the input equation into a computational graph automatically.\n",
    "\n",
    "\n",
    "\n",
    "## 5.2 Core Classes and Important Attributes\n",
    "- `AutoDiff`: The core class for the scalar implementation of forward mode automatic differentiation is the `AutoDiff` class. Defining an instance of the class allows the user to store and update function and derivative values. When an instance of the `AutoDiff` class is defined, the user is able to easily store,and update function and derivative values. These function and derivative values are attributes of the `AutoDiff` class.\n",
    "\n",
    "**Note:** The following two attributes are considered private. The user should not access these directly.\n",
    "- `_val`: Stores the function value\n",
    "- `_der`: Stores the derivative value; defaults to `1.0` if the user does not pass in a second argument to `AutoDiff` when defining a new instance\n",
    "\n",
    "There exist getter and setter methods to access current values and set new values for `_val` and `_der`.\n",
    "\n",
    "\n",
    "including attributes: inputs (the list of input nodes), op (the associated operator object), const_attr (the add or multiply constant) and name (node name for debugging purposes). \n",
    "\n",
    "### Methods\n",
    "We have implemented in this milestone the following methods for the `AutoDiff` class:\n",
    "\n",
    "- `__init__`:initialize a AutoDiff class object, regardless of the user input, with values and derivatives stored as float.\n",
    "- `__repr__`: overload the print format, prints self in the form of Function value: [val] Derivative value: [der]) \n",
    "- `__neg__`: overload negitive function\n",
    "- `__add__`: overload add function to handle addition of AutoDiff class objects and addition of AutoDiff and non-AutoDiff objects\n",
    "- `__radd__`: preserve addition commutative property.\n",
    "- `__sub__`: overload sub function to handle addition of AutoDiff class objects and addition of AutoDiff and non-AutoDiff objects\n",
    "- `__rsub__`: preserve substraction commutative property.\n",
    "- `__mul__`: overload multiplication function to handle addition of AutoDiff class objects and addition of AutoDiff and non-AutoDiff objects\n",
    "- `__rmul__`: preserve multiplication commutative property.\n",
    "- `__truediv__`: overload division function to handle addition of AutoDiff class objects and addition of AutoDiff and non-AutoDiff objects\n",
    "- `__rtruediv__`: preserve division commutative property. \n",
    "-` __pow__`: extend power functions to AutoDiff class objects, power times should be a integer.\n",
    "\n",
    "\n",
    "### Functions\n",
    "\n",
    "- `sin(x)`\n",
    "- `cos(x)`\n",
    "- `tan(x)`\n",
    "- `exp(x)`\n",
    "- `log(x)`\n",
    "\n",
    "The funtions above take as input a AutoDiff class object and return a new AutoDiff class object with its value and derivative updated according to the elementary function it corresponds to using the chain rule.\n",
    "\n",
    "## 5.3 External Dependencies\n",
    "- Numpy: is used for mathematical calculations. \n",
    "\n",
    "- pytest: it provides us a systematic way to test \n",
    "\n",
    "- TravisCI and Codecov: they are our test suites.\n",
    "\n",
    "## 5.4 Elementary Functions\n",
    "The following elementary functions are supported: sine, cosine, tangent, exponential, and logarithm. Recall from elementary calculus that the derivatives of these functions are the following.\n",
    "\\begin{align}\n",
    "    \\frac{d}{dx} \\sin x &= \\cos x\\\\\n",
    "    \\frac{d}{dx} \\cos x &= -\\sin x\\\\\n",
    "    \\frac{d}{dx} \\tan x &= \\sec^2 x = \\frac{1}{\\cos^2 x}, \\quad x \\neq \\frac{\\pi}{2}\\\\\n",
    "    \\frac{d}{dx} \\exp x &= \\exp x\\\\\n",
    "    \\frac{d}{dx} \\ln x &= \\frac{1}{x}, \\quad x > 0\n",
    "\\end{align}\n",
    "Note that $\\tan x$ and $\\ln x$ will raise a `ValueError` if the user passes in a function value that is outside the valid domain.\n",
    "\n",
    "The methods that execute these elementary functions are defined in the `AD` module and work as follows. Each method takes in a single argument that is an instance of the `AutoDiff` class and returns a new instance of the `AutoDiff` class with the appropriate function and derivative values. To avoid rounding error, the `check_tol` method in the `AD` module compares the calculated values with their rounded counterparts. For example, suppose we wish to calculate $\\tan x$ and its derivative at $x=\\pi/4$. The `check_tol` method ensures that `AD.tan(AD.AutoDiff(np.pi/4))` returns `Function value: 1.0, Derivative value: 2.0` rather than `Function value: 1.0, Derivative value: 1.999999...`.\n",
    "\n",
    "\n",
    "### Not implemented - Vectorizaton\n",
    "In this current release, our automatic differentiation package can only handle scalar to scalar functions. In the future, we will extend this work to include the more general category of scalar to scalar functions. This extension will primarily preserve the structure of the code but will result in rewriting the main functions to be able to deal with vector inputs.\n",
    "\n",
    "We are going to support gradient computation for vectors and matrixes.   For simplicity, we only discuss how to compute the gradient of the matrix because the vector is only the special case of a matrix.  Operations between matrixes have two classes. The first one is the element-wise operation. For this kind of operation, we do not have to change the code for scalar and only need to replace scalar values with `np.array`. The second one is the matrix-wise operation like matrix multiplication. For this kind of operation, we use `np.array` to represent the value of variables again. However, we need to reimplement the operations because they have different formulations of the gradient. For example, if the operation is matrix multiplication between matrix A and B, the returned gradient from output to A and B are $dA = dY \\times B^T$ and $dB = A^T \\times dY$.\n",
    "Reverse mode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Extension - Reverse Mode\n",
    "\n",
    "Apart from the detailed implementation for reverse mode in part 5, we would like to give description of some high-level insight of our extension. For the reverse mode, we need to build the computational graph and figure out the sequence of computation correctly to get the right gradient.\n",
    "### Ststic V.S. Dynamic Graph\n",
    "Basically, there are two strategies of building the computational graph, static graph and dynamic graph. The former computes value and gradient after finishing graph and the latter do the computation dynamically. Although dynamic is easier to debug and more user-friendly, static graph is more clear to understand reverse mode and more suitable for this course. So we choose static graph based reverse mode as our main framework.\n",
    "### Attributes and Data Structure Used in Class Node\n",
    "A computational graph consists of multiple nodes. Each node has two main attributes, self.inputs and self.op which indicate inputs of current node and the operation between inputs. It is worth mentioning that nodes are created by their own operation. The input list is a python list whose elements are inputs of the current node. For example, if `node_3 =  ad.Op_1(node_2, node_1)` , we have `node_3.inputs = [node_2, node_1]` and `node_3.op = ad.Op_1`. Several operations are implemented, such as add, add_by_const, mul, mul_by_const, tan, sin and so on. \n",
    "### How to Support Higher-Order Gradient\n",
    "To better support higher order gradient computation, gradients of leaf and intermediate variable are also represented by nodes. As a result, our computational graph not only has nodes representing values of variable during forward propagation, but also has nodes representing gradients of variable during back propagation. If users want to compute higher order gradient, they can easily add new nodes representing gradient of nodes of low order gradient. With this mechanism, we can compute any order gradient.\n",
    "### Traveral on Graph\n",
    "How to find the path of creating nodes of gradients in the graph is also quite important to get correct results efficiently. Given a list of nodes, we use a post-order Depth First Search (DFS) to get a topological sort list of nodes ending in our target nodes. The reverse of this list is the right sequence to add nodes of gradients.\n",
    "\n",
    "### Efficiency of Our Implementation\n",
    "As we mentioned above, we use nodes to represent both values and gradients of variables in the same graph. This implementation has two advantages. First, to compute a gradient node, we may have to access to some associated value nodes. With our implementation, we do not have to recompute the results of these value nodes. Second, we do post-order DFS on the computational graph and try to find a topology-sorted list of nodes which means that we only need to use part of the graph to compute the result of a certain node we care about instead of executing the whole graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Future Work\n",
    "\n",
    "So far, our GuruDiff can handle both scalar and vector functions in both forward and reverse mode. Our basic implementation for the two modes actually combines the advantage of PyTorch and TensorFlow. In reverse mode, we could calculate pluriderivative. Based on this, one thing we want to achieve in the future is to add more applications for users, like optimization, root finding etc. Another thing is that we hope this package is friendly to who are not so familiar with coding and can be served as a wonderful educational tool in courses. So we would like to develop a graphical user interface to let customers play with the package as a calculator or visualize the process the want to figure out.\n",
    "## 7.1 Optimization\n",
    "\n",
    "### Background\n",
    "\n",
    "Nowadays, machine learning and deep neural network are widely used in different areas like computer science, economics, physics, biology etc. One key point for most of the algotithms is using Gradient Descent to find the local minimum of a function f. Specifically, a user may want to find optimal weights to minimize a loss function. \n",
    "\n",
    "BFGS, short for “Broyden–Fletcher–Goldfarb–Shanno algorithm”, seeks a stationary point of a function, i.e. where the gradient is zero. In quasi-Newton methods, the Hessian matrix of second derivatives is not computed. Instead, the Hessian matrix is approximated using updates specified by gradient evaluations (or approximate gradient evaluations).\n",
    "\n",
    "Here is a pseudocode of the implementation of BFGS.<img src=\"./img/op1.jpeg\" width = \"400\" alt=\"name\" align=center />\n",
    "\n",
    "\n",
    "### Method\n",
    "\n",
    "In future work, we want to add some method to the optimization module:\n",
    "`GradientDescent:` solve for a local minimum of a function. It takes following input:\n",
    "- f: in machine learning applications, this should be the loss/objective function in a format of a numpy array.\n",
    "\n",
    "- x: initial guess for function f\n",
    "\n",
    "- iters: the maximum number of iterations to run\n",
    "\n",
    "- tol: a threshold for the algorithm to terminate when difference is smaller than tol\n",
    "\n",
    "and outputs a numpy array of the minimum that the algorithm found and the Jacobian value at the specified root. In addition, we hope it also return the value and gradient path for the finding process, thus we can define plot function that can visualize the gradient descent process. This is very helpful to make the process intuitional and friendly for beginners in machine learning field. \n",
    "\n",
    "## 7.2 GUI\n",
    "\n",
    "Hopefully, we want to develop a web application for the user to easy use the package. We may need:\n",
    "- For web development, we would use `Flask`, a micro web framework, which is suitable for a small team to complete the implementation of a feature-rich small website and easily add customized functions.\n",
    "- For GUI (Graphical User Interface), we may choose `Vue.js`, a JavaScript frame- work for building user interfaces and single-page applications. Because it offers many API (Application Program Interface) to integrate with existing projects and is easy to get started. It is better in code reuse compared to frameworks like `jQuery`.\n",
    "\n",
    "The web page should include a calculator for users to pass the input and see the value and derivative of their defined function. This will be convinient for our daily use.\n",
    "\n",
    "It should also could display all the results when user using our package, th\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
