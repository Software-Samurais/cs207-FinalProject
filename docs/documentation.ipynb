{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
    "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS207 Project Milestone 2  - @Software-Samurais</h1> \n",
    "    \n",
    "### Group 3: \n",
    "#### Erick Ruiz, Jingyuan Liu, Kailas Amin, Simon (Xin) Dong\n",
    "\n",
    "\n",
    "\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Introduction\n",
    "\n",
    "The increasing importance of computational models in science and business alongside the slowing pace of advances in computing hardware has increased the need for efficient and accurate evaluations of derivatives. Many important applications such as simulation, opti- mization, and neural networks rely on repeated differentiation of complex functions.\n",
    "\n",
    "Before the advent of automatic differentiation (AD) the primary method for derivative evaluation was the method of finite differences (FD), where the function to be evaluated is effectively treated as black box oracle.1 As the FD method is effectively sampling, the granularity (i.e. step size) of the algorithm can introduce error effects if it is either too large or too small, but even at the perfect medium, f′(x) evaluations cannot reach machine precision. The alternative approach, fully symbolic differentiation (SD), is cumbersome and inefficient in many cases. In the case of a complex computer program, the size of the symbolic expression could grow to outrageous size and cause significant inefficiency.\n",
    "\n",
    "The approach of algorithmic differentiation seeks to find the best of both worlds, with machine precision and easy evaluation. This is done by repeated evaluation of the chain rule at a point stored in a table called the computational trace. Thus rather than storing to full symbolic expression, an AD code only needs to apply the chain rule to a specific evalua- tion, representable by a single variable. This approach allows us to achieve the accuracy of symbolic approaches while drastically reducing the cost of evaluation.\n",
    "Within the umbrella of automatic differentiation, we seek to implement the forward mode which evaluates the intermediate results directly in an inside out manner. Other approaches such as reverse mode also have specific advantages especially in the fields of machine learning and artificial intelligence– or in any context in which the number of inputs dominates the number of outputs.\n",
    "\n",
    "The method of automatic differentiation, sometimes also referred to as algorithmic differ- entiation, addresses the weaknesses of the finite difference method by providing a systematic way to calculate derivatives numerically to arbitrary precision. The goal of AutoDiff is to implement the forward mode of automatic differentiation, as it is a relevant feature that even some mainstream machine learning libraries, such as PyTorch, lack.\n",
    "\n",
    "\n",
    "# 2 Background\n",
    "Understanding the concept of a derivative is crucial to all aspiring and\n",
    "practicing scientists, engineers, and mathematicians. It is one of the\n",
    "first concepts introduced in first-year calculus courses at all\n",
    "universities. The idea is simple. Given a function, $f(x)$, how can we\n",
    "quantify the rate of change of the function due to an infinitesimal\n",
    "change, $\\Delta x$, in the argument, $x$? The answer is typically given\n",
    "in terms of the limit definition of the derivative.\n",
    "\\begin{equation}\n",
    "f'(x) = \\lim_{\\Delta x\\rightarrow 0} \\frac{f(x+\\Delta x)-f(x)}{\\Delta x}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "While equation (1) holds for any function, in practice, it is\n",
    "easier to calculate derivatives analytically according to a set of\n",
    "rules. However, obtaining an analytical expression for the derivative\n",
    "becomes exceedingly difficult if the function of interest is composed of\n",
    "many elementary functions.\\\n",
    "In any case of a derivative evaluating program, we will need to create a\n",
    "code which takes as inputs both a function and evaluation point and then\n",
    "return the derivative at that point. There are many approaches to\n",
    "perform this calculation.\\\n",
    "For example, consider the following function.\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\exp\\left[\\frac{\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}{\\cos(3x^5)}\\right]\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Calculating the first derivative would result\n",
    "in the following expression. \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    f'(x) &= \\exp\\left[\\frac{\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}{\\cos(3x^5)}\\right]\n",
    "    \\sec^2(3x^5)\\dots\\nonumber\\\\\n",
    "    &\\qquad\\times\\left\\{\n",
    "    \\frac{\\cos(3x^5)}{2\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}\n",
    "    \\left[3x^2-\\frac{1}{x}+8x\\cos(4x^2)\\right]\\dots\\right.\\nonumber\\\\\n",
    "    &\\qquad\\qquad+\n",
    "    \\left.\\vphantom{\\frac{\\cos(3x^5)}{2\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}}\n",
    "    15x^4\\sin(3x^5)\\sqrt{x^3 - \\ln x + \\sin(4x^2)}\n",
    "    \\right\\}\\end{aligned}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Although feasible, successive calculations\n",
    "become more and more complex, and in practice, the quantity to be\n",
    "differentiated may not be a function in closed-form but rather a set of\n",
    "measurements or values given as a one-dimensional vector of numbers. In\n",
    "that case, equation (1) can be approximated using the finite difference\n",
    "method, which replaces an infinitesimal change in the argument for a\n",
    "finite change. To show how this works, let us write the Taylor series\n",
    "expansion of an arbitrary function, $f(x)$, at the point $x+h$.\n",
    "\n",
    "\\begin{equation}\n",
    "f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\dots\n",
    "    \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "Keeping only terms of $\\mathcal{O}(h)$ leaves us\n",
    "with \n",
    "\n",
    "\\begin{equation}\n",
    "f(x) \\approx f(h) + hf'(x),\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "which we can rearrange to write an\n",
    "approximate expression for the derivative, $f'(x)$.\n",
    "\n",
    "\\begin{equation}\n",
    "f'(x) \\approx \\frac{f(x+h)-f(x)}{h}\n",
    "    \\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "The finite change, $h$, is called the step size, and\n",
    "equation (6) is known as the forward difference. Its geometric interpretation is\n",
    "described in Figure 1.<img src=\"./img/figure1.jpeg\" width = \"600\" alt=\"name1\" align=center />\n",
    "\n",
    "Although the finite difference method is useful and easy to implement,\n",
    "its accuracy can vary depending on the step size that is chosen. Suppose\n",
    "we wish to approximate the derivative of $f(x)=\\ln x$ using the forward\n",
    "difference method described in equation (6) using step\n",
    "sizes $h=\\{10^{-1},\\,\n",
    "10^{-7},\\,10^{-15}\\}$. This is rather unnecessary because the analytical\n",
    "derivative is just $f'(x) = 1/x$, but this example will serve to\n",
    "illustrate the drawbacks of the finite difference method. At\n",
    "$h=10^{-1}$, the numerical derivative is inaccurate because the step\n",
    "size is too large, making the calculations susceptible to truncation\n",
    "error. Conversely, at $h=10^{-15}$, the forward difference method also\n",
    "gives inaccurate results because the calculations can only be\n",
    "represented to a finite precision by the hardware in use. Hence,\n",
    "rounding error also affects the stability of the finite difference\n",
    "method.\n",
    "\n",
    "In order to evaluate the derivative via forward mode AD we first\n",
    "construct a computational graph which encodes the composition and\n",
    "dependence of sub-function evaluations. It is important to note that\n",
    "each elementary function evaluation needs to be understood at a symbolic\n",
    "level! After constructing the graph, the chain rule is simply applied\n",
    "successively to evaluations at a single point which then generates a\n",
    "table known as the computational trace. To illustrate the concept,\n",
    "consider the following example, adapted from *Evaluating derivatives:\n",
    "principles and techniques of algorithmic differentiation* by Griewank\n",
    "and Walther.\n",
    "\\begin{equation}\n",
    "f(x,y) = \\left[\\sin\\left(\\frac{x}{y}\\right) + \\frac{x}{y} - \\exp(y)\\right]\n",
    "    \\left[\\frac{x}{y} - \\exp(y)\\right]\n",
    "    \\tag{7}\n",
    "\\end{equation}\n",
    "A function of two arguments (i.e. variables $x$ and\n",
    "$y$) like the one in equation (7) can be\n",
    "evaluated at point by replacing the arguments with numerical values. The\n",
    "series of calculations needed to carry out the evaluation can be\n",
    "visualized as a computation graph, as shown in Figure 2.<img src=\"./img/figure2.png\" width = \"800\" alt=\"name\" align=center />\n",
    "The graph helps visualize the order of the\n",
    "computations, and it also establishes the dependence of successive\n",
    "calculations on previous ones.\\\n",
    "Although in many cases, the AD approach is given the entire function as\n",
    "input, this is not always the case. This necessitates the need for a\n",
    "\\\"seed\\\" value. In the basic case when the entire function is given, we\n",
    "can simply initialize the computational trace (discussed below) as 1.\n",
    "However, given the need for modularity which respects the chain rule, it\n",
    "is also possible to input a different seed, allowing for the integration\n",
    "of AD onto a sub segment of the function.\n",
    "\n",
    "\\\n",
    "The procedure for constructing such a graph is the following. Define a\n",
    "node for each of the inputs using a new variable. For this example, we\n",
    "will let $x_1 = x$ and $x_2 = y$. From there, any successive nodes may\n",
    "accept two inputs at maximum, and each node represents a new\n",
    "calculation. For example, in the computational graph shown in Figure 3, the node $x_3$ represents the calculation\n",
    "$x_1/x_2$, which later becomes an input in successive nodes. While the\n",
    "computational graph is useful for visualizing the entire computation\n",
    "procedure, the computational trace is useful for storing values as the\n",
    "computation is carried out. The computational trace for\n",
    "$f(1.5000, 0.5000)$ is given in Table 1![Table1](img/table1.png) In reference to the above, we can note that in\n",
    "this trace, the seed is 1 and thus suppressed as is often notated. In\n",
    "addition, it is important to note that depending on what implementation\n",
    "of forward mode the exact target computed can be slightly different. In\n",
    "particular, the vectorized extension of our milestone 2 code computes\n",
    "the Jacobian.\n",
    "\n",
    "Beyond the basic forward mode, there exists other implementations of AD\n",
    "in the so called \"reverse mode\\\". This approach can increase efficiency\n",
    "significantly in cases when the number of inputs is far greater than the\n",
    "number of outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 How to use\n",
    "## 3.1 <span id=\"jump\">Installation</span>\n",
    "\n",
    "The `github` url to the project is https://github.com/Software-Samurais/cs207-FinalProject.\n",
    "\n",
    "We provide two ways for our package installation: PyPI and GitHub.\n",
    "### Install from PyPI\n",
    "\n",
    "We have already uploaded our GuruDiff package to PyPI. To use the GuruDiff package, users should first run the commands provided below to install our package via pip and import it.\n",
    "\n",
    "\n",
    "- Install GuruDiff using pip:\n",
    "```shell\n",
    "pip install GuruDiff\n",
    "```\n",
    "- Import GuruDiff package to use\n",
    "```Python\n",
    "import GuruDiff.AD as AD\n",
    "```\n",
    "\n",
    "### Install from GitHub\n",
    "- Download the package from GitHub to your folder via these commands (in the terminal)\n",
    "\n",
    "```shell\n",
    "# Clone the repo\n",
    "git clone https://github.com/Software-Samurais/cs207-FinalProject.git\n",
    "\n",
    "```\n",
    "\n",
    "- Create a virtual environment and activate it (for Mac and Ubutun)\n",
    "\n",
    "```shell\n",
    "# If you don't have virtualenv, install it\n",
    "sudo easy_install virtualenv\n",
    "# Create virtual environment\n",
    "virtualenv env\n",
    "# Activate your virtual environment\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "- Install the necessary dependencies :\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- Run module tests (in the root directory):\n",
    "\n",
    "```shell\n",
    "# Run module tests if you like\n",
    "python -m pytest ./test/test_forward.py ./test/test_reverse.py\n",
    "```\n",
    "\n",
    "## 3.2 Getting Started with `GuruDiff`\n",
    "\n",
    "The following is a demonstration of `GuruDiff`'s capabilities to help the user hit the ground running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "# Adds a relative file path to this notebook\n",
    "sys.path.append(\"..\") \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import autodiff.AD as AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Variables and Performing Elementary Operations\n",
    "### Scalar Variables\n",
    "We may define a scalar variable as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = AD.Var(np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we when a value for the derivative is not specified, a default value of `1.0` is assigned automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function value:\n",
       "3.141592653589793\n",
       "Derivative value:\n",
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a variable's most relevant information\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also allow the user to specify a value for the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function value:\n",
       "1.0\n",
       "Derivative value:\n",
       "3.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = AD.Var(1.0, 3.0)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we may use scalar variables to define custom scalar functions composed of standard elementary operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function value:\n",
       "8.86963026268246\n",
       "Derivative value:\n",
       "6.283022813997788"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**2 + AD.cos(x) + 0.5*AD.exp(-x**2)\n",
    "\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    Since <code>x</code> is a forward mode variable, we must use the implementations of elementary functions defined in the <code>AD</code> module instead of the standard Numpy methods.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar Variables Using Arrays\n",
    "Suppose we wish to define a scalar variable containing an array as the function value to visualize both the function and derivative values (e.g. plot $\\sin x$). The `Var` class is versatile enough to handle this directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instance of the `Var` class with an array as the function value\n",
    "x = AD.Var(np.linspace(-np.pi, np.pi, 100))\n",
    "\n",
    "# Function to visualize\n",
    "f1 = AD.sin(x)\n",
    "\n",
    "# Plot function and derivative\n",
    "plt.figure()\n",
    "plt.plot(x.val, f1.val, label=\"$\\sin x$\")\n",
    "plt.plot(x.val, f1.der, label=\"$\\cos x$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Function Variables\n",
    "Suppose we have a vector-valued function of the form $\\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^m$, where $\\mathbf{x} \\in \\mathbb{R}^n$. Defining a variable for a vector function can be accomplished in a similar manner as before. As an example, consider the vector-valued function\n",
    "\\begin{equation}\n",
    "    \\mathbf{f}(\\mathbf{x})\n",
    "    =\n",
    "    \\begin{pmatrix}\n",
    "        \\sin x\\\\\n",
    "        4y + z^3\n",
    "    \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "Calculating $\\nabla\\mathbf{f}$ yields the following Jacobian.\n",
    "\\begin{equation}\n",
    "    \\nabla\\mathbf{f} \n",
    "    = \n",
    "    \\begin{pmatrix}\n",
    "        \\cos x & 0 & 0\\\\\n",
    "        0 & 4 & 3z^2\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function values:\n",
       "[1. 4.]\n",
       "Jacobian:\n",
       "[[6.123234e-17 0.000000e+00 0.000000e+00]\n",
       " [0.000000e+00 4.000000e+00 1.200000e+01]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define scalar variables and their respective unit vectors\n",
    "x = AD.Var(np.pi/2, [1, 0, 0])\n",
    "y = AD.Var(3, [0, 1, 0])\n",
    "z = AD.Var(-2, [0, 0, 1])\n",
    "\n",
    "# Use the scalar variables to define vector-valued function\n",
    "f = AD.Var([AD.sin(x), 4*y + z**3])\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function value:\n",
       "[6.]\n",
       "Derivative value:\n",
       "[5.]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ℝ1→ℝ1\n",
    "x = AD.Var([np.pi/2])\n",
    "f = AD.sin(x) + 5 * AD.tan(x/2)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function value:\n",
       "3.718281828459045\n",
       "Gradient:\n",
       "[0.         2.71828183]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ℝm→ℝ1\n",
    "x = AD.Var([np.pi/2], [1, 0])\n",
    "y = AD.Var([1], [0, 1])\n",
    "f = AD.sin(x) + AD.exp(y)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function values:\n",
       "[1.        2.4674011]\n",
       "Derivative values:\n",
       "[[0.        ]\n",
       " [3.14159265]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ℝ1→ℝn\n",
    "x = AD.Var([np.pi/2], [1])\n",
    "f = AD.Var([AD.sin(x), x ** 2])\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function values:\n",
       "[1. 4.]\n",
       "Jacobian:\n",
       "[[ 0.  0.  0.]\n",
       " [ 0.  4. 12.]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ℝm→ℝn\n",
    "x = AD.Var([np.pi/2], [1, 0, 0])\n",
    "y = AD.Var([3], [0, 1, 0])\n",
    "z = AD.Var([-2], [0, 0, 1])\n",
    "f = AD.Var([AD.sin(x), 4 * y + z ** 3])\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Newton's Method in Higher Dimensions\n",
    "\n",
    "Newton's method is an iterative technique for finding the zeros of nonlinear functions. For scalar functions of a single variable (e.g. $f(x)$), the iterations may be calculated as follows.\n",
    "\\begin{equation}\n",
    "    x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n",
    "\\end{equation}\n",
    "Here, $x_{k+1}$ is the next iteration, $x_k$ is the previous iteration, and $f(x)$ is the function of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same technique can be generalized to encompass multivariate vector-valued functions. Suppose $\\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^m$ is the differentiable function of interest. Then, the system of equations we wish to solve is $\\mathbf{f}(\\mathbf{x}) = \\mathbf{0}$. Newton's method calculates the solution iteratively as follows.\n",
    "\\begin{equation}\n",
    "    \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\delta\\mathbf{x}^{(k)}\n",
    "\\end{equation}\n",
    "Here, the second term on the right-hand side is defined as\n",
    "\\begin{equation}\n",
    "    \\delta\\mathbf{x}^{(k)} = -\\mathbf{J}\\left(\\mathbf{x}^{(k)}\\right)^{-1}\\mathbf{f}\\left(\\mathbf{x}^{(k)}\\right),\n",
    "\\end{equation}\n",
    "where $\\mathbf{J}\\left(\\mathbf{x}^{(k)}\\right)$ is the Jacobian evaluated at $\\mathbf{x}^{(k)}$. In general, it is computationally expensive to explicitly calculate the inverse of a matrix, so instead, we obtain $\\delta\\mathbf{x}^{(k)}$ by solving the system\n",
    "\\begin{equation}\n",
    "    \\mathbf{J}\\left(\\mathbf{x}^{(k)}\\right)\\delta\\mathbf{x}^{(k)} = -\\mathbf{f}\\left(\\mathbf{x}^{(k)}\\right)\n",
    "\\end{equation}\n",
    "via Gaussian elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, x0, tol=1e-8, max_iter=100):\n",
    "    \"\"\"Newton's method for multivariate vector functions\n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "    f (callable): vector function of interest\n",
    "    x0 (array_like): initial guess in the form [x, y, z]\n",
    "    tol (float): tolerance value; by default set to 1e-8\n",
    "    max_iter (int): maximum number of iterations to execute\n",
    "    \n",
    "    Returns:\n",
    "    ========\n",
    "    xn (array): solution, if the algorithm converges\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial guess\n",
    "    xn = x0\n",
    "    \n",
    "    for n in range(max_iter):\n",
    "        \n",
    "        # Evaluate f\n",
    "        fn = f(*xn)\n",
    "            \n",
    "        # Calculate difference vector\n",
    "        df = np.linalg.solve(fn.der, -fn.val)\n",
    "        \n",
    "        if abs(df).any() < tol:\n",
    "            print(f\"Found a solution after {n} iterations.\")\n",
    "            return np.around(xn, decimals=3)\n",
    "        \n",
    "        if np.linalg.det(fn.der) == 0:\n",
    "            raise ValueError(\"Jacobian not invertible. No solution.\")\n",
    "        \n",
    "        # Update the guess\n",
    "        xn = xn + df\n",
    "    \n",
    "    print(\"Exceeded maximum number of iterations.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adapted from section 3.1 of \"The Mathematics of Nonlinear Programming\" by Peressini, Sullivan and Uhl\n",
    "def f(x,y,z):\n",
    "    X = AD.Var(x, [1, 0, 0])\n",
    "    Y = AD.Var(y, [0, 1, 0])\n",
    "    Z = AD.Var(z, [0, 0, 1])\n",
    "    \n",
    "    Fx = X**2 + Y**2 + Z**2 - 3\n",
    "    Fy = X**2 + Y**2 - Z - 1\n",
    "    Fz = X + Y + Z - 3\n",
    "    \n",
    "    val = np.asarray([Fx.val, Fy.val, Fz.val])\n",
    "    jac = np.asarray([Fx.der, Fy.der, Fz.der])\n",
    "    \n",
    "    return AD.Var(val, jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a solution after 27 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton(f, [1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Software Organization\n",
    "\n",
    "## 4.1 Directory Structure\n",
    "The package's directory will be structured as follows:\n",
    "``` py\n",
    "Ccs207-FinalProject\n",
    "    /autodiff                                           #Back-end source code\n",
    "\t    __init__.py                                     #Initialization\n",
    "\t    forward.py                                      #Forward mode: main file\n",
    "        reverse.py                                      #Reverse mode: extension feature\n",
    "\t/test                                               #Test cases\n",
    "\t\ttest_forward.py\n",
    "        test_reverse.py\n",
    "\t/docs                                               #Documentation and records\n",
    "\t\tmilestone1.pdf\n",
    "        milestone2.ipynb\n",
    "        documentation.ipynb                             #Final documentation for the package\n",
    "\t/demo                                               #Simple demos for user\n",
    "    .travis.yml                                         #Setting up Travis CI \n",
    "    setup.py                                            #Releasing package\n",
    "    requirements.txt                                    #Packages on which the program depends\n",
    "    README.md                                           #Introduction for the project\n",
    "```\n",
    "##  4.2 Basic Modules and functionality\n",
    "\n",
    "We choose to have two module published: forward mode for auto-differentiation in `AD` module and reverse mode in `reverse` module.\n",
    "\n",
    "- **forward**: This module contains our custom library for auto differentiation in forward mode. \n",
    "    - The `AD` module includes a `Var` class that contains values and derivatives, which is used to define a variable for auto differentiation. In the class, we override the operators like `__repr__`, `__neg__`, `__add__`, `__radd__`, `__sub__`, `__rsub__`, `__mul__`, `__rmul__`, `__truediv__`, `__rtruediv__`, `__pow__` and comparison operators like `__eq__` and `__ne__`.\n",
    "    - In addition, we define class-specific functions including Trig functions (sine, cosine, tangent), Inverse trig functions (arcsine, arccosine, arctangent), Exponentials with any base, Hyperbolic functions (sinh, cosh, tanh), Logistic function, Logarithms with any base, Square root. Thus the user could use our defined math function easily (as we use numpy).\n",
    "\n",
    "- **reverse**: This module contains our custom library for auto differentiation in reverse mode. It includes several class: Node, Op (including sevaral sub-class for specific operator), Executor. \n",
    "    - `Node` class defines the Node in a computation graph used in reverse mode, which contains list of input nodes, operator, constant, and node name .In the class, we also override the basic operators and comparison operators.\n",
    "    - `Op` class defines the operations performed on nodes. It could return a Node which is created by this op, real values, or a Node for the gradient computation in the reverse mode. It is the parent class for several operaor class like `SinOp`, `ExpOp` etc. Those classes inherited from `Op` define Elementary Functions for the operations that our forward mode implements.\n",
    "    - `Executor` class computes values for a given subset of nodes in a computation graph. It takes list of nodes whose values need to be computed as input.\n",
    "\n",
    "    \n",
    "## 4.3 Test Suite\n",
    "### Where do the tests live? \n",
    "\n",
    "All test files will be placed in the test folder.\n",
    "\n",
    "- `test_forward`: It includes tests for scalar or vector functions to ensure that the AD module properly calculates values of different functions and gradients with respect to scalar or vector inputs.\n",
    "\n",
    "- `test_reverse`: This is a test suite for our extention feature: reverse mode. It ensures the reverse model properly works.\n",
    "\n",
    "### How are they run? \n",
    "\n",
    "Users can run the test suite by running in the root folder:\n",
    "```shell\n",
    "python -m pytest ./test/test_forward.py #seperately test forward\n",
    "python -m pytest ./test/test_reverse.py #seperately test reverse\n",
    "python -m pytest ./test/test_forward.py ./test/test_reverse.py #test all\n",
    "```\n",
    "### How are they integrated?\n",
    "\n",
    "We would utilize Travis CI and CodeCov to make the development process more reliable and professional. \n",
    "\n",
    "- `Travis CI` is used as a distributed CI (Continuous Integration) tools to build and automate test the project.\n",
    "     \n",
    "- `CodeCov` is used for test results analysis (eg. measuring test code coverage) and visualization.    \n",
    "\n",
    "\n",
    "\n",
    "## 4.4 Software Package and Distribution:\n",
    "### Package distribution\n",
    "We will package our software using `PyPI` (Python Package Index) for release. Write and run ’setup.py’ to package the software and upload it to the distribution server, thus people in community could easily download our package by ’pip install’.\n",
    "### Version Control\n",
    "We will take Version Control into consideration according to the standard in Python Enhancement Proposal (PEP) 386. With version control, we can tell the user what changes we made and set clear boundaries for where those changes occurred.\n",
    "\n",
    "## 4.5 How to install the package\n",
    "\n",
    "User can download and install our package either using pip or manually clone from github as mentioned in [3.1 Installation](#jump).\n",
    "\n",
    "- For consumers, We suggest to follow installation procedure with pip, which is simple and easy to achieve.\n",
    "\n",
    "- For developers, they could also install from pip but they may care about the source code and the detailed implementation. If so, they could clone they repository from github and even contribute to the new version of the package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5 Implementation\n",
    "\n",
    "## 5.1 Core Data Structures\n",
    "\n",
    "- **List and Numpy array** - Users can pass their variable in the format of list, and we will transform it to numpy array for further calculation. In forward mode, we use numpy array to rrack how many independent variables have been created and do corresponding operations. In reverse mode, we can also dynamically track the descendants of this variable in the computational graph.\n",
    "\n",
    "\n",
    "## 5.2 Core Classes and Important Attributes\n",
    "There are one core classes in the forward mode: the Var class and three core classes in the reverse mode: Node, Op (including sevaral sub-class for specific operator), Executor.\n",
    "### Var\n",
    "The core class for forward mode is the `Var`. Defining an instance of the class allows the user to denote a scalar or vector variable with function value and derivative value. \n",
    "\n",
    "It has two **attributes**:\n",
    "\n",
    "**Note:** The following two attributes are considered private. The user should not access these directly.\n",
    "\n",
    "There exist getter and setter methods to access current values and set new values for `_val` and `_der`.\n",
    "\n",
    "To cover vector-to-vector cases, we implement our self._var and self._der as float or numpy arrays. the constructor checks whether the values and derivatives are integers, floats, or lists, and transforms them into float or numpy arrays automatically.\n",
    "- `_val`: Stores the function value\n",
    "- `_der`: Stores the derivative value; defaults to `1.0` or corresponding shape of numpy array with all `1.0` if the user does not pass in a second argument to `Var` when defining a new instance.\n",
    "\n",
    "We have implemented the following methods for the `Var` class as basic operators and comparison operators, that take in 1 Var object, or 2 Var object, or 1 Var object and 1 constant, and return a new Var object:\n",
    "\n",
    "- `__init__`:initialize a Var class object, regardless of the user input, with values and derivatives stored as float or numpy array.\n",
    "- `__repr__`: overload the print format, prints self in the intuitive form of Function value: [val] Derivative/Gradient/Jacobian: [der]). Derivative would refer to simple scalar variables, Gradient would refer to scalar functions of multiple variables, and Jacobian is reserved for vector functions.\n",
    "- `__neg__`: overload negitive function\n",
    "- `__add__`: overload add function to handle addition of Var class objects and addition of Var and non-Var objects\n",
    "- `__radd__`: preserve addition commutative property.\n",
    "- `__sub__`: overload sub function to handle addition of Var class objects and addition of Var and non-Var objects\n",
    "- `__rsub__`: preserve substraction commutative property.\n",
    "- `__mul__`: overload multiplication function to handle addition of Var class objects and addition of v and non-AutoDiff objects\n",
    "- `__rmul__`: preserve multiplication commutative property.\n",
    "- `__truediv__`: overload division function to handle addition of Var class objects and addition of Var and non-Var objects\n",
    "- `__rtruediv__`: preserve division commutative property. \n",
    "-` __pow__`: extend power functions to Var class objects, the power could be int, float or Var class objects.\n",
    "- `__eq__`: return True if two Var objects have the same value and derivative (including value and shape), False otherwise.\n",
    "- `__ne__`: return False if two Var objects have the same value and derivative (including value and shape), True otherwise.\n",
    " \n",
    "### Node\n",
    "\n",
    "`Node` class define a Node in a computation graph when using reverse mode. It helps to dynamically track the descendants of this variable in the computational graph.\n",
    "\n",
    "It has four **attributes**:\n",
    "- `inputs`: the list of input nodes.\n",
    "- `op`: the associated op object, e.g. add_op object if this node is created by adding two other nodes.\n",
    "- `const_attr`: the add or multiply constant, e.g. self.const_attr=5 if this node is created by x+5.\n",
    "- `name`: node name for debugging purposes.\n",
    "\n",
    "as we do in forwad mode with Var class, we also overload the following methods to deal with the situation when operating between a Node object and a int/float number: `__str__` (this will return the node name), `__neg__`, \n",
    "`__add__`, `__radd__` , `__sub__`, `__rsub__`, `__mul__`, `__rmul__`, `__truediv__`, `__rtruediv__`, `__pow__`, `__eq__`, `__ne__`.\n",
    "\n",
    "### Op\n",
    "\n",
    "Op represents operations performed on nodes. This is a parent class for specific operator. It has no attribute but three important methods.\n",
    "\n",
    "- `__call__`: Create a new node and associate the op object with the node. This return a Node which is created by this op.\n",
    "- `compute`: the only function which will return real values which will be used for computation. It takes node (node that performs the compute) and input_vals (values of input nodes) as inputs and returns an output value of the node.\n",
    "- `gradient`: return a Node for the gradient computation for the reverse mode. It takes node (node that performs the gradient) and output_grad (value of output gradient summed from children nodes' contributions) as inputs and returns a list of gradient contributions to each input node respectively.\n",
    "\n",
    "The former two methods actually are implemented in child class inherited from the Op class. We define following classes as operators:\n",
    "\n",
    "- `AddOp`: Op to element-wise add two nodes.\n",
    "- `AddByConstOp`: Op to element-wise add a nodes by a constant.\n",
    "- `MulOp`: Op to element-wise multiply two nodes.\n",
    "- `MulByConstOp`: Op to element-wise multiply a nodes by a constant.\n",
    "- `MatMulOp`: Op to matrix multiply two nodes.\n",
    "\n",
    "For code reusability, we don't define SubOp and DivideOp because x SubOp y can be subsitituted by x + (- y) and x DividedOp y is actually x * ($y^{-1}$).\n",
    "\n",
    "similar to forward mode, we define elementary function as operator classes with element-wise functions: `SinOp`,\n",
    "`CosOp`, `TanOp`, `SinhOp`, `CoshOp`, `TanhOp`, `ArcSinOp`, `ArcCosOp`, `ArcTanOp`, `ExpOp`, `LogOp`, `PowerOp`, `LogisticOp`.\n",
    "\n",
    "There are some sepecial Op child classes:\n",
    "- `PlaceholderOp`: Op to feed value to a nodes. The call function creates a variable node.\n",
    "- `ZerosLikeOp`: Op that represents a constant np.zeros_like. The call function creates a node that represents a np.zeros array of same shape as node_A.\n",
    "- `OnesLikeOp`: Op that represents a constant np.ones_like. The call function creates a node that represents a np.ones array of same shape as node_A.\n",
    "\n",
    "\n",
    "### Executor\n",
    "`Executor` class computes values for a given subset of nodes in a computation graph.\n",
    "\n",
    "It has one attribute:\n",
    "- `eval_node_list`: list of nodes whose values need to be computed.\n",
    "\n",
    "and one method:\n",
    "\n",
    "- `run`: Computes values of nodes in eval_node_list given computation graph. It takes feed_dict (list of variable nodes whose values are supplied by user) as input and returns a list of values for nodes in eval_node_list.\n",
    "\n",
    "Our reverse mode implementation is efficient which absorbs the insight of TenforFlow. The executor class is similar to what we done when using session in TenforFlow.\n",
    "\n",
    "\n",
    "\n",
    "## 5.3 External Dependencies\n",
    "- Numpy: is used for mathematical calculations and deal with value and derivative vectors. \n",
    "\n",
    "- pytest: it provides us a systematic way to test \n",
    "\n",
    "- setuptools, wheel, twine: setuptools, wheel are used to package our repo before we distribute it and twine are use to distribute our package to PyPI.\n",
    "\n",
    "- TravisCI and Codecov: they are our test suites.\n",
    "\n",
    "\n",
    "## 5.4 Elementary Functions\n",
    "we define elementary functions who takes Var object as input and return a Var object with its value and derivative updated according to the elementary function it corresponds to using the chain rule. Including \n",
    "- Trig functions: `sin(x)`, `cos(x)` , `tan(x)`\n",
    "\n",
    "- Inverse trig functions: `arcsine(x)`, `arccosine(x)`, `arctangent(x)` \n",
    "\n",
    "- Hyperbolic functions: `sinh(x)`, `cosh(x)`, `tanh(x)`\n",
    "\n",
    "- Square root: `sqrt(x)`, Logistic function: `logistic(x)`\n",
    "\n",
    "- Exponentials with any base: `exp(x, base)`, Logarithms with any base `log(x, base)`. x would be a Var obeject and base would be int. If user do not denote base, it will be set to natural base as default.\n",
    "\n",
    "Thus the user could use our defined math function easily (as we use numpy). Note that elementary function will raise a `ValueError` if the user passes in a function value that is outside the valid domain. For example: log(0), arcsin(3) etc.\n",
    "\n",
    "To avoid rounding error, the `check_tol` method in the `forward` module compares the calculated values with their rounded counterparts. For example, suppose we wish to calculate $\\tan x$ and its derivative at $x=\\pi/4$. The `check_tol` method ensures that `AD.tan(AD.AutoDiff(np.pi/4))` returns `Function value: 1.0, Derivative value: 2.0` rather than `Function value: 1.0, Derivative value: 1.999999...`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Extension - Reverse Mode\n",
    "\n",
    "Apart from the detailed implementation for reverse mode in part 5, we would like to give description of some high-level insight of our extension. For the reverse mode, we need to build the computational graph and figure out the sequence of computation correctly to get the right gradient.\n",
    "### Ststic V.S. Dynamic Graph\n",
    "Basically, there are two strategies of building the computational graph, static graph and dynamic graph. The former computes value and gradient after finishing graph and the latter do the computation dynamically. Although dynamic is easier to debug and more user-friendly, static graph is more clear to understand reverse mode and more suitable for this course. So we choose static graph based reverse mode as our main framework.\n",
    "### Attributes and Data Structure Used in Class Node\n",
    "A computational graph consists of multiple nodes. Each node has two main attributes, self.inputs and self.op which indicate inputs of current node and the operation between inputs. It is worth mentioning that nodes are created by their own operation. The input list is a python list whose elements are inputs of the current node. For example, if `node_3 =  ad.Op_1(node_2, node_1)` , we have `node_3.inputs = [node_2, node_1]` and `node_3.op = ad.Op_1`. Several operations are implemented, such as add, add_by_const, mul, mul_by_const, tan, sin and so on. \n",
    "### How to Support Higher-Order Gradient\n",
    "To better support higher order gradient computation, gradients of leaf and intermediate variable are also represented by nodes. As a result, our computational graph not only has nodes representing values of variable during forward propagation, but also has nodes representing gradients of variable during back propagation. If users want to compute higher order gradient, they can easily add new nodes representing gradient of nodes of low order gradient. With this mechanism, we can compute any order gradient.\n",
    "### Traveral on Graph\n",
    "How to find the path of creating nodes of gradients in the graph is also quite important to get correct results efficiently. Given a list of nodes, we use a post-order Depth First Search (DFS) to get a topological sort list of nodes ending in our target nodes. The reverse of this list is the right sequence to add nodes of gradients.\n",
    "\n",
    "### Efficiency of Our Implementation\n",
    "As we mentioned above, we use nodes to represent both values and gradients of variables in the same graph. This implementation has two advantages. First, to compute a gradient node, we may have to access to some associated value nodes. With our implementation, we do not have to recompute the results of these value nodes. Second, we do post-order DFS on the computational graph and try to find a topology-sorted list of nodes which means that we only need to use part of the graph to compute the result of a certain node we care about instead of executing the whole graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Future Work\n",
    "\n",
    "So far, our GuruDiff can handle both scalar and vector functions in both forward and reverse mode. Our basic implementation for the two modes actually combines the advantage of PyTorch and TensorFlow. In reverse mode, we could calculate pluriderivative. Based on this, one thing we want to achieve in the future is to add more applications for users, like optimization, root finding etc. Another thing is that we hope this package is friendly to who are not so familiar with coding and can be served as a wonderful educational tool in courses. So we would like to develop a graphical user interface to let customers play with the package as a calculator or visualize the process the want to figure out.\n",
    "## 7.1 Optimization\n",
    "\n",
    "### Background\n",
    "\n",
    "Nowadays, machine learning and deep neural network are widely used in different areas like computer science, economics, physics, biology etc. One key point for most of the algotithms is using Gradient Descent to find the local minimum of a function f. Specifically, a user may want to find optimal weights to minimize a loss function. \n",
    "\n",
    "BFGS, short for “Broyden–Fletcher–Goldfarb–Shanno algorithm”, seeks a stationary point of a function, i.e. where the gradient is zero. In quasi-Newton methods, the Hessian matrix of second derivatives is not computed. Instead, the Hessian matrix is approximated using updates specified by gradient evaluations (or approximate gradient evaluations).\n",
    "\n",
    "Here is a pseudocode of the implementation of BFGS.<img src=\"./img/op1.jpeg\" width = \"400\" alt=\"name\" align=center />\n",
    "\n",
    "\n",
    "### Method\n",
    "\n",
    "In future work, we want to add some method to the optimization module:\n",
    "`GradientDescent:` solve for a local minimum of a function. It takes following input:\n",
    "- f: in machine learning applications, this should be the loss/objective function in a format of a numpy array.\n",
    "\n",
    "- x: initial guess for function f\n",
    "\n",
    "- iters: the maximum number of iterations to run\n",
    "\n",
    "- tol: a threshold for the algorithm to terminate when difference is smaller than tol\n",
    "\n",
    "and outputs a numpy array of the minimum that the algorithm found and the Jacobian value at the specified root. In addition, we hope it also return the value and gradient path for the finding process, thus we can define plot function that can visualize the gradient descent process. This is very helpful to make the process intuitional and friendly for beginners in machine learning field. \n",
    "\n",
    "## 7.2 GUI\n",
    "\n",
    "Hopefully, we want to develop a web application for the user to easy use the package. We may need:\n",
    "- For web development, we would use `Flask`, a micro web framework, which is suitable for a small team to complete the implementation of a feature-rich small website and easily add customized functions.\n",
    "- For GUI (Graphical User Interface), we may choose `Vue.js`, a JavaScript frame- work for building user interfaces and single-page applications. Because it offers many API (Application Program Interface) to integrate with existing projects and is easy to get started. It is better in code reuse compared to frameworks like `jQuery`.\n",
    "\n",
    "The web page should include a calculator for users to pass the input and see the value and derivative of their defined function. This will be convenient for our daily use.\n",
    "\n",
    "It should also could display all the results when user using our package, thus it could serve as a good demo in classes for teachers to guide beginners.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
