{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Features\n",
    "These are some proposals for future features in the AD package. Some of these will be implemented in the final version!\n",
    "\n",
    "## Vectorizaton\n",
    "In this current release, our automatic differentiation package can only handle scalar to scalar functions. In the future, we will extend this work to include the more general category of scalar to scalar functions. This extension will primarily preserve the structure of the code but will result in rewriting the main functions to be able to deal with vector inputs. \n",
    "\n",
    "## Intermediate Results\n",
    "As written, the code will only output the final result of any multistage calculation. In the final version, one possible feature would be to store/output intermediate results to test debugging and/or numerical stability. In order to implement this, we would likely need to change many of the basic functions or set up a higher level function or decorator which saves the input of a function. \n",
    "\n",
    "## Optimization\n",
    "We could write an application to solve (potentially constrained) continuous optimization problems. In contrast with the above two extensions, This method would most probably result in a largely seperate codebase which builds on and calls the top level code from our AD library. \n",
    "\n",
    "## Basic Neural Networks\n",
    "In a similar vein to optimization, we could implement basic neural networks. This approach may need to be coupled with reverse mode as both high dimensional optimization and neural networks are much faster when implemented using reverse mode. \n",
    "\n",
    "## Root Finding\n",
    "We could write an application to find the roots of continuous functions. This method would most probably result in a largely seperate codebase which builds on and calls the top level code from our AD library. \n",
    "\n",
    "## Reverse Mode\n",
    "Another options is to implement the reverse mode. Although not fundementally increasing the capacity of the code, implementation of a reverse mode would allow the user to effectively differentiate functions with a large number of inputs. For applications such as NNs, root finding, and optimization, the joint implementation of reverse mode could yield a huge speed up.\n",
    "\n",
    "## Non Differentiable Functions\n",
    "One final potential option is support for non-differentible functions, especially loops and if statements. In order to implement this, we would likely simply add to the library of elementary functions and return a null for non differentibilities. This approach would likely be somewhat complicated as simple operator overloading would not suffice. In this case, we would need to implement some sort of parser to transfer the code into piecewise functions. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
