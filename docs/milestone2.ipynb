{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
    "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS207 Project Milestone 2  - @Software-Samurais</h1> \n",
    "    \n",
    "### Group 3: \n",
    "#### Erick Ruiz, Jingyuan Liu, Kailas Amin, Simon (Xin) Dong\n",
    "\n",
    "\n",
    "\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Introduction\n",
    "\n",
    "The increasing importance of computational models in science and business alongside the slowing pace of advances in computing hardware has increased the need for efficient and accurate evaluations of derivatives. Many important applications such as simulation, opti- mization, and neural networks rely on repeated differentiation of complex functions.\n",
    "\n",
    "Before the advent of automatic differentiation (AD) the primary method for derivative evaluation was the method of finite differences (FD), where the function to be evaluated is effectively treated as black box oracle.1 As the FD method is effectively sampling, the granularity (i.e. step size) of the algorithm can introduce error effects if it is either too large or too small, but even at the perfect medium, f′(x) evaluations cannot reach machine precision. The alternative approach, fully symbolic differentiation (SD), is cumbersome and inefficient in many cases. In the case of a complex computer program, the size of the symbolic expression could grow to outrageous size and cause significant inefficiency.\n",
    "\n",
    "The approach of algorithmic differentiation seeks to find the best of both worlds, with machine precision and easy evaluation. This is done by repeated evaluation of the chain rule at a point stored in a table called the computational trace. Thus rather than storing to full symbolic expression, an AD code only needs to apply the chain rule to a specific evalua- tion, representable by a single variable. This approach allows us to achieve the accuracy of symbolic approaches while drastically reducing the cost of evaluation.\n",
    "Within the umbrella of automatic differentiation, we seek to implement the forward mode which evaluates the intermediate results directly in an inside out manner. Other approaches such as reverse mode also have specific advantages especially in the fields of machine learning and artificial intelligence– or in any context in which the number of inputs dominates the number of outputs.\n",
    "\n",
    "The method of automatic differentiation, sometimes also referred to as algorithmic differ- entiation, addresses the weaknesses of the finite difference method by providing a systematic way to calculate derivatives numerically to arbitrary precision. The goal of AutoDiff is to implement the forward mode of automatic differentiation, as it is a relevant feature that even some mainstream machine learning libraries, such as PyTorch, lack.\n",
    "\n",
    "# 2 Background\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 How to use\n",
    "\n",
    "## 3.1 <span id=\"jump\">Installation</span>\n",
    "\n",
    "The `github` url to the project is https://github.com/Software-Samurais/cs207-FinalProject.\n",
    "####  - Download the package from GitHub to your folder via these commands (in the terminal)\n",
    "Assuming that the user already has the latest version of `Python` 3, `Git` and a package manager of choice installed (like `pip`).\n",
    "\n",
    "\n",
    "\n",
    "```shell\n",
    "# Clone the repo\n",
    "git clone https://github.com/Software-Samurais/cs207-FinalProject.git\n",
    "\n",
    "```\n",
    "\n",
    "#### - Create a virtual environment and activate it (for Mac and Ubutun)\n",
    "\n",
    "```shell\n",
    "# If you don't have virtualenv, install it\n",
    "sudo easy_install virtualenv\n",
    "# Create virtual environment\n",
    "virtualenv env\n",
    "# Activate your virtual environment\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "#### -  Install the necessary dependencies :\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "#### - Run module tests (in the root directory)\n",
    "```shell\n",
    "# Run module tests if you like\n",
    "python -m pytest ./test/test_forward.py\n",
    "```\n",
    "\n",
    "## 3.2 Basic Demo\n",
    "\n",
    "In this part, we will give a basic demo for a quick guide to the user. \n",
    "\n",
    "Once AutoDiff is installed, the user must import it to be able to use it. The user will have the option to either import the entire library or to choose only a subset of modules, classes, or methods to import.\n",
    "\n",
    "In this demo, we import the package from autodiff directory and give an alias AD to the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package from parent directory \n",
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "import autodiff.AD as AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Denote constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Function value: 3.0, Derivative value: 1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constant (scalar or vector): User do not need to initialize derivative, it will set to 1.0\n",
    "x1 = AD.AutoDiff(3.0)\n",
    "# Each object has two attributes: val and der \n",
    "print(x1.val, x1.der)\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Denote scalar variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function value: 2.5, Derivative value: 2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User can pass value and derivative of a variable\n",
    "x2 = AD.AutoDiff(2.5, 2.0)\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary Operations\n",
    "\n",
    "We can perform basic elementary operations on `AutoDiff` objects, just as we would with numbers. For example, addition, subtraction, multiplication, division, and power operations are all supported. Simply define an instance of the `AutoDiff` class to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = AD.AutoDiff(3.0)\n",
    "y = AD.AutoDiff(2.0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: -3.0, Derivative value: -1.0\n",
      "Function value: -2.0, Derivative value: -0.5\n"
     ]
    }
   ],
   "source": [
    "# Negative\n",
    "print(-x)\n",
    "print(-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 6.0, Derivative value: 1.0\n",
      "Function value: 5.0, Derivative value: 1.5\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "print(x + 3.)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 2.0, Derivative value: 1.0\n",
      "Function value: 1.0, Derivative value: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Subtraction\n",
    "print(x - 1.)\n",
    "print(x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 9.0, Derivative value: 3.0\n",
      "Function value: 6.0, Derivative value: 3.5\n"
     ]
    }
   ],
   "source": [
    "# Multiplication\n",
    "print(3.* x)\n",
    "print(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 1.0, Derivative value: 0.3333333333333333\n",
      "Function value: 0.3333333333333333, Derivative value: -0.1111111111111111\n",
      "Function value: 1.5, Derivative value: 0.125\n"
     ]
    }
   ],
   "source": [
    "# Division\n",
    "print(x / 3.)\n",
    "print(1 / x)\n",
    "print(x / y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 9.0, Derivative value: 6.0\n",
      "Function value: 8.0, Derivative value: 6.0\n"
     ]
    }
   ],
   "source": [
    "# Power\n",
    "print(x ** 2)\n",
    "print(y ** 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigonometric Functions, Exponentials, and Logarithms\n",
    "\n",
    "Other basic elementary operations, such as trigonometric functions, exponentials, and natural logarithms are also supported. Example uses of each of these are shown in the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 0.1411200080598672, Derivative value: -0.9899924966004454\n",
      "Function value: -0.27941549819892586, Derivative value: 3.360596003276281\n"
     ]
    }
   ],
   "source": [
    "# Sine\n",
    "print(AD.sin(x))\n",
    "print(AD.sin(x*y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: -0.9899924966004454, Derivative value: -0.1411200080598672\n",
      "Function value: 0.5403023058681398, Derivative value: -0.42073549240394825\n"
     ]
    }
   ],
   "source": [
    "# Cosine\n",
    "print(AD.cos(x))\n",
    "print(AD.cos(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: -0.1425465430742778, Derivative value: 1.020319516942427\n",
      "Function value: 14.10141994717172, Derivative value: 24.98125556581156\n"
     ]
    }
   ],
   "source": [
    "# Tangent\n",
    "print(AD.tan(x))\n",
    "print(AD.tan(x/y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 20.085536923187668, Derivative value: 20.085536923187668\n",
      "Function value: 403.42879349273517, Derivative value: 605.1431902391028\n"
     ]
    }
   ],
   "source": [
    "# Exponential\n",
    "print(AD.exp(x))\n",
    "print(AD.exp(y)**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 1.0986122886681098, Derivative value: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Logarithm\n",
    "print(AD.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Functions\n",
    "\n",
    "Customized functions can be defined using these elementary operations. For example, suppose we wish to work with the function\n",
    "\\begin{equation}\n",
    "    f(x) = \\exp\\left(\\sin x^2\\right) - x^4 + \\log(x). \n",
    "\\end{equation}\n",
    "We may define $f(x)$ using the standard Python conventions. However, instead of using the `exp` and `sin` methods from Numpy, we must use the `exp` and `sin` methods defined in the `AD` module because only the latter accept instances of the `AutoDiff` class as inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function value: -78.39137437130643, Derivative value: -115.9215797663472"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return AD.exp(AD.sin(x**2)) - x**4 + AD.log(x)\n",
    "\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Application: Newton's Method\n",
    "\n",
    "Given an equation, $f(x)$, Newton's method allows us to find solutions to $f(x)=0$ iteratively. Starting with an initial guess, $x_0$, we evaluate the function and its derivative at $x_0$. If $|f(x_0)| < \\epsilon$, where $\\epsilon$ is some tolerance value much less than one, then the iteration stops. Otherwise, the value of the next guess, $x_1$, is obtained as follows.\n",
    "\\begin{equation}\n",
    "    x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}\n",
    "\\end{equation}\n",
    "Following the same scheme, the $n$-th iteration can be written as \n",
    "\\begin{equation}\n",
    "    x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})},\n",
    "\\end{equation}\n",
    "where $x_{n-1}$ is the guess at the previous iteration.\n",
    "\n",
    "Since Newton's method makes use of the derivative of the function of interest, we may use automatic differentiation to implement Newton's method! Traditionally, we might consider using the finite difference method to approximate $f'(x)$ or define it explicitly in our routine. There are drawbacks to each of these approaches. The finite difference method is easy to implement, but its accuracy is strongly dependent on choosing the right step size. If $f(x)$ is a simple function, then explicitly defining $f'(x)$ is not a problem. However, this is not always possible. With the `AD` module, there is no need to approximate or explicitly define $f'(x)$. As we operate on our initial guess, $x_0$, the value of the derivative is automatically calculated as well, making it easy to implement Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, x0, tol=1e-16, max_iter=100):\n",
    "    \"\"\"Solves f(x) = 0 using Newton's method.\n",
    "    \n",
    "    Args:\n",
    "    =========\n",
    "    f (function): Function of interest\n",
    "    x0 (float): Initial guess\n",
    "    tol (float): Tolerance value\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    =========\n",
    "    xn.val (float): Solution to f(x) = 0 if it exists\n",
    "                    None if xn.der is zero or if the maximum number of \n",
    "                    iterations is reached without satisfying the stopping  \n",
    "                    criteria\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial guess\n",
    "    xn = x0\n",
    "    \n",
    "    for n in range(max_iter):\n",
    "        \n",
    "        # Calculate f(xn) and f'(xn) using the AutoDiff class\n",
    "        fn = f(xn)\n",
    "        \n",
    "        # Stop iterating if |f(xn)| is less than the tolerance value and return \n",
    "        # the solution, xn\n",
    "        if abs(fn.val) < tol:\n",
    "            print(f\"Found a solution after {n} iterations.\")\n",
    "            return xn.val\n",
    "        \n",
    "        # Check if the derivative is zero\n",
    "        if fn.der == 0:\n",
    "            raise ValueError(\"Encountered zero derivative. No solution.\")\n",
    "            \n",
    "        # Update guess\n",
    "        xn = xn - fn.val/fn.der\n",
    "        \n",
    "    # Stop iterating if no solution is found within the allowed number of \n",
    "    # iterations\n",
    "    print(\"Exceeded maximum number of iterations.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a solution after 6 iterations.\n",
      "Solution: 1.618033988749895\n"
     ]
    }
   ],
   "source": [
    "# Function of interest\n",
    "def f(x):\n",
    "    return x**2-x-1\n",
    "\n",
    "# Initial guess   \n",
    "x0 = AD.AutoDiff(1.0)\n",
    "\n",
    "print(f\"Solution: {newton(f, x0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Software Organization\n",
    "\n",
    "## 4.1 Directory Structure\n",
    "The package's directory will be structured as follows:\n",
    "``` py\n",
    "Ccs207-FinalProject\n",
    "\t__init__.py                                         #Initialization\n",
    "    /autodiff                                           #Back-end source code\n",
    "\t    /config                                         #Configuration for the project\n",
    "\t    AD.py\n",
    "    /gui                                                #Front-end source code\n",
    "        /dist                                           #Static css, js etc.\n",
    "        /template                                       #Web html files\n",
    "        /img                                            #Images used for font-end\n",
    "    /utils                                              #Preprocessing scripts\n",
    "\t/test                                               #Test cases\n",
    "\t\ttest_vector.py\n",
    "\t\ttest_forward.py\n",
    "        test_reverse.py\n",
    "\t/docs                                               #Documentation and records\n",
    "\t\tmilestone1.pdf\n",
    "        milestone2.ipynb\n",
    "\t/demo\n",
    "    requirements.txt                                    #Packages on which the program depends\n",
    "    README.md                                           #Introduction for the project\n",
    "```\n",
    "##  4.2 Basic Modules and functionality\n",
    "\n",
    "So far, we have simple forward mode for auto-differentiation in `AD` module.\n",
    "\n",
    "- `AD`: This module contains our custom library for autodifferentiation. \n",
    "    - It includes functionality for a AutoDiff class that contains values and derivatives. In the class, we override the operator like `__repr__`, `__neg__`, `__add__`, `__radd__`, `__sub__`, `__rsub__`, `__mul__`, `__rmul__`, `__truediv__`, `__rtruediv__`, `__pow__`.\n",
    "    - In addition, we define class-specific functions e.g., `sine`, `cosine`, `tangent`, `power`, `exponentiation`, `logarithm`. Thus the user could use our defined math function easily (as we use numpy).\n",
    "\n",
    "We plan to include reverse mode and other modules for application in the later work:\n",
    "\n",
    "- `optimize`: This module is designed to perform optimization. Users can define a custom function to optimize. For example, the first n−1 columns denote the features of the data, and the final column represents the labels. The user could specify the function to optimize as “mse”. Then, the function will find a local minimum of the mean squared error objective function. Finally, the module allows for static and animated plots for visualization.\n",
    "\n",
    "- `rootfinding`: This module is designed to find roots of a given function. It includes Newton’s method. It also allows the user to visualize static or animated results for visualization.\n",
    "\n",
    "- `utils`: This module is designed for parsing the input, preprocessing and start main program.\n",
    "    \n",
    "## 4.3 Test Suite\n",
    "Coding is the fundamental part of software development. Equally significant is build and testing. \n",
    "- We would utilize Travis CI and CodeCov to make the development process more reliable and professional. \n",
    "\n",
    "     `Travis CI` is used as a distributed CI (Continuous Integration) tools to build and automate test the project.\n",
    "     \n",
    "     `CodeCov` is used for test results analysis (eg. measuring test code coverage) and visu- alization.\n",
    "\n",
    "We have already set up these integrations, with badges included in the README.md, showing that build passed 100%, and coverage 100% .\n",
    "- Users can run the test suite by running in the root folder:\n",
    "```shell\n",
    "python -m pytest ./test/test_forward.py\n",
    "```\n",
    "- All test files will be placed in the test folder.\n",
    "\n",
    "    `test_forward`: It includes tests for scalar functions to ensure that the AD module properly calculates values of scalar functions and gradients with respect to scalar inputs.\n",
    "    \n",
    "    we plan to add more test files in future work:\n",
    "\n",
    "    `test_rootfinding`: This is a test suite for rootfinding.\n",
    "    \n",
    "    `test_optimize`: This is a test suite for optimization.\n",
    "\n",
    "\n",
    "\n",
    "## 4.4 Software Package and Distribution:\n",
    "### Package distribution\n",
    "We will package our software using `PyPI` (Python Package Index) for release. Write and run ’setup.py’ to package the software and upload it to the distribution server, thus people in community could easily download our package by ’pip install’.\n",
    "### Version Control\n",
    "We will take Version Control into consideration according to the standard in Python Enhancement Proposal (PEP) 386. With version control, we can tell the user what changes we made and set clear boundaries for where those changes occurred.\n",
    "### Framework\n",
    "- For web development, we would use `Flask`, a micro web framework, which is suitable for a small team to complete the implementation of a feature-rich small website and easily add customized functions.\n",
    "- For GUI (Graphical User Interface), we may choose `Vue.js`, a JavaScript frame- work for building user interfaces and single-page applications. Because it offers many API (Application Program Interface) to integrate with existing projects and is easy to get started. It is better in code reuse compared to frameworks like `jQuery`.\n",
    "\n",
    "## 4.5 How to install the package\n",
    "\n",
    "At this point, our package isn't on `PyPI` (will distribute later). User can download and install our package manually as mentioned in [3.1 Installation](#jump).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5 Implementation\n",
    "\n",
    "## Forward Mode\n",
    "\n",
    "### Core Data Structures\n",
    "- `AutoDiff`: For the scalar implementation of forward mode automatic differentiation, `AutoDiff` objects are the core data structures. When an instance of the `AutoDiff` class is defined, the user is able to easily store,and update function and derivative values. These function and derivative values are attributes of the `AutoDiff` class.\n",
    "\n",
    "### Core Classes\n",
    "- `AutoDiff`: The core class for the scalar implementation of forward mode automatic differentiation is the `AutoDiff` class. Defining an instance of the class allows the user to store and update function and derivative values. \n",
    "\n",
    "### Important Attributes\n",
    "**Note:** The following two attributes are considered private. The user should not access these directly.\n",
    "- `_val`: Stores the function value\n",
    "- `_der`: Stores the derivative value; defaults to `1.0` if the user does not pass in a second argument to `AutoDiff` when defining a new instance\n",
    "\n",
    "There exist getter and setter methods to access current values and set new values for `_val` and `_der`.\n",
    "\n",
    "### External Dependencies\n",
    "- Numpy is used for mathematical calculations. \n",
    "\n",
    "### Elementary Functions\n",
    "The following elementary functions are supported: sine, cosine, tangent, exponential, and logarithm. Recall from elementary calculus that the derivatives of these functions are the following.\n",
    "\\begin{align}\n",
    "    \\frac{d}{dx} \\sin x &= \\cos x\\\\\n",
    "    \\frac{d}{dx} \\cos x &= -\\sin x\\\\\n",
    "    \\frac{d}{dx} \\tan x &= \\sec^2 x = \\frac{1}{\\cos^2 x}, \\quad x \\neq \\frac{\\pi}{2}\\\\\n",
    "    \\frac{d}{dx} \\exp x &= \\exp x\\\\\n",
    "    \\frac{d}{dx} \\ln x &= \\frac{1}{x}, \\quad x > 0\n",
    "\\end{align}\n",
    "Note that $\\tan x$ and $\\ln x$ will raise a `ValueError` if the user passes in a function value that is outside the valid domain.\n",
    "\n",
    "The methods that execute these elementary functions are defined in the `AD` module and work as follows. Each method takes in a single argument that is an instance of the `AutoDiff` class and returns a new instance of the `AutoDiff` class with the appropriate function and derivative values. To avoid rounding error, the `check_tol` method in the `AD` module compares the calculated values with their rounded counterparts. For example, suppose we wish to calculate $\\tan x$ and its derivative at $x=\\pi/4$. The `check_tol` method ensures that `AD.tan(AD.AutoDiff(np.pi/4))` returns `Function value: 1.0, Derivative value: 2.0` rather than `Function value: 1.0, Derivative value: 1.999999...`.\n",
    "\n",
    "## 5.2 Planning work\n",
    "\n",
    "### Not implemented - Reverse mode\n",
    "\n",
    "For the reverse mode, we need to build the computational graph and figure out the sequence of computation correctly to get the right gradient.\n",
    "\n",
    "Basically, there are two strategies of building the computational graph, static graph and dynamic graph. The former computes value and gradient after finishing graph and the latter do the computation dynamically. Although dynamic is easier to debug and more user-friendly, static graph is more clear to understand reverse mode and more suitable for this course. So we choose static graph based reverse mode as our main framework.\n",
    "\n",
    "A computational graph consists of multiple nodes. Each node has two main attributes, self.inputs and self.op which indicate inputs of current node and the operation between inputs. It is worth mentioning that nodes are created by their own operation.\n",
    "\n",
    "Several operations are implemented, such as add, add_by_const, mul, mul_by_const, tan, sin and so on. To better support higher order gradient computation, gradients of leaf and intermediate variable are also represented by nodes. As a result, our computational graph not only has nodes representing values of variable during forward propagation, but also has nodes representing gradients of variable during back propagation. If users want to compute higher order gradient, they can easily add new nodes representing gradient of nodes of low order gradient. With this mechanism, we can compute any order gradient.\n",
    "\n",
    "How to find the path of creating nodes of gradients in the graph is also quite important to get correct results efficiently. Given a list of nodes, we use a post-order Depth First Search (DFS) to get a topological sort list of nodes ending in our target nodes. The reverse of this list is the right sequence to add nodes of gradients.\n",
    "\n",
    "### Plan on implementing - Vectorizaton, Optimization, Root Finding etc.\n",
    "More details could be found at [6 Future Features](#6).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 <span id='6'>Future Features </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some proposals for future features in the AD package. Some of these will be implemented in the final version!\n",
    "### Vectorizaton\n",
    "In this current release, our automatic differentiation package can only handle scalar to scalar functions. In the future, we will extend this work to include the more general category of scalar to scalar functions. This extension will primarily preserve the structure of the code but will result in rewriting the main functions to be able to deal with vector inputs.\n",
    "\n",
    "### Intermediate Results\n",
    "As written, the code will only output the final result of any multistage calculation. In the final version, one possible feature would be to store/output intermediate results to test debugging and/or numerical stability. In order to implement this, we would likely need to change many of the basic functions or set up a higher level function or decorator which saves the input of a function.\n",
    "\n",
    "### Optimization\n",
    "We could write an application to solve (potentially constrained) continuous optimization problems. In contrast with the above two extensions, This method would most probably result in a largely seperate codebase which builds on and calls the top level code from our AD library.\n",
    "\n",
    "### Basic Neural Networks\n",
    "In a similar vein to optimization, we could implement basic neural networks. This approach may need to be coupled with reverse mode as both high dimensional optimization and neural networks are much faster when implemented using reverse mode.\n",
    "\n",
    "### Root Finding\n",
    "We could write an application to find the roots of continuous functions. This method would most probably result in a largely seperate codebase which builds on and calls the top level code from our AD library.\n",
    "\n",
    "### Reverse Mode\n",
    "Another options is to implement the reverse mode. Although not fundementally increasing the capacity of the code, implementation of a reverse mode would allow the user to effectively differentiate functions with a large number of inputs. For applications such as NNs, root finding, and optimization, the joint implementation of reverse mode could yield a huge speed up.\n",
    "\n",
    "### Non Differentiable Functions\n",
    "One final potential option is support for non-differentible functions, especially loops and if statements. In order to implement this, we would likely simply add to the library of elementary functions and return a null for non differentibilities. This approach would likely be somewhat complicated as simple operator overloading would not suffice. In this case, we would need to implement some sort of parser to transfer the code into piecewise functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
