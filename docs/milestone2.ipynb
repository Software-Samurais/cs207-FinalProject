{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
    "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS207 Project Milestone 2  - @Software-Samurais</h1> \n",
    "    \n",
    "### Group 3: \n",
    "#### Erick Ruiz, Jingyuan Liu, Kailas Amin, Simon (Xin) Dong\n",
    "\n",
    "\n",
    "\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Introduction\n",
    "\n",
    "The increasing importance of computational models in science and business alongside the slowing pace of advances in computing hardware has increased the need for efficient and accurate evaluations of derivatives. Many important applications such as simulation, opti- mization, and neural networks rely on repeated differentiation of complex functions.\n",
    "\n",
    "Before the advent of automatic differentiation (AD) the primary method for derivative evaluation was the method of finite differences (FD), where the function to be evaluated is effectively treated as black box oracle.1 As the FD method is effectively sampling, the granularity (i.e. step size) of the algorithm can introduce error effects if it is either too large or too small, but even at the perfect medium, f′(x) evaluations cannot reach machine precision. The alternative approach, fully symbolic differentiation (SD), is cumbersome and inefficient in many cases. In the case of a complex computer program, the size of the symbolic expression could grow to outrageous size and cause significant inefficiency.\n",
    "\n",
    "The approach of algorithmic differentiation seeks to find the best of both worlds, with machine precision and easy evaluation. This is done by repeated evaluation of the chain rule at a point stored in a table called the computational trace. Thus rather than storing to full symbolic expression, an AD code only needs to apply the chain rule to a specific evalua- tion, representable by a single variable. This approach allows us to achieve the accuracy of symbolic approaches while drastically reducing the cost of evaluation.\n",
    "Within the umbrella of automatic differentiation, we seek to implement the forward mode which evaluates the intermediate results directly in an inside out manner. Other approaches such as reverse mode also have specific advantages especially in the fields of machine learning and artificial intelligence– or in any context in which the number of inputs dominates the number of outputs.\n",
    "\n",
    "The method of automatic differentiation, sometimes also referred to as algorithmic differ- entiation, addresses the weaknesses of the finite difference method by providing a systematic way to calculate derivatives numerically to arbitrary precision. The goal of AutoDiff is to implement the forward mode of automatic differentiation, as it is a relevant feature that even some mainstream machine learning libraries, such as PyTorch, lack.\n",
    "\n",
    "\n",
    "# 2 Background\n",
    "Understanding the concept of a derivative is crucial to all aspiring and\n",
    "practicing scientists, engineers, and mathematicians. It is one of the\n",
    "first concepts introduced in first-year calculus courses at all\n",
    "universities. The idea is simple. Given a function, $f(x)$, how can we\n",
    "quantify the rate of change of the function due to an infinitesimal\n",
    "change, $\\Delta x$, in the argument, $x$? The answer is typically given\n",
    "in terms of the limit definition of the derivative.\n",
    "\\begin{equation}\n",
    "f'(x) = \\lim_{\\Delta x\\rightarrow 0} \\frac{f(x+\\Delta x)-f(x)}{\\Delta x}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "While equation (1) holds for any function, in practice, it is\n",
    "easier to calculate derivatives analytically according to a set of\n",
    "rules. However, obtaining an analytical expression for the derivative\n",
    "becomes exceedingly difficult if the function of interest is composed of\n",
    "many elementary functions.\\\n",
    "In any case of a derivative evaluating program, we will need to create a\n",
    "code which takes as inputs both a function and evaluation point and then\n",
    "return the derivative at that point. There are many approaches to\n",
    "perform this calculation.\\\n",
    "For example, consider the following function.\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\exp\\left[\\frac{\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}{\\cos(3x^5)}\\right]\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Calculating the first derivative would result\n",
    "in the following expression. \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    f'(x) &= \\exp\\left[\\frac{\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}{\\cos(3x^5)}\\right]\n",
    "    \\sec^2(3x^5)\\dots\\nonumber\\\\\n",
    "    &\\qquad\\times\\left\\{\n",
    "    \\frac{\\cos(3x^5)}{2\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}\n",
    "    \\left[3x^2-\\frac{1}{x}+8x\\cos(4x^2)\\right]\\dots\\right.\\nonumber\\\\\n",
    "    &\\qquad\\qquad+\n",
    "    \\left.\\vphantom{\\frac{\\cos(3x^5)}{2\\sqrt{x^3 - \\ln x + \\sin(4x^2)}}}\n",
    "    15x^4\\sin(3x^5)\\sqrt{x^3 - \\ln x + \\sin(4x^2)}\n",
    "    \\right\\}\\end{aligned}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Although feasible, successive calculations\n",
    "become more and more complex, and in practice, the quantity to be\n",
    "differentiated may not be a function in closed-form but rather a set of\n",
    "measurements or values given as a one-dimensional vector of numbers. In\n",
    "that case, equation (1) can be approximated using the finite difference\n",
    "method, which replaces an infinitesimal change in the argument for a\n",
    "finite change. To show how this works, let us write the Taylor series\n",
    "expansion of an arbitrary function, $f(x)$, at the point $x+h$.\n",
    "\n",
    "\\begin{equation}\n",
    "f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\dots\n",
    "    \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "Keeping only terms of $\\mathcal{O}(h)$ leaves us\n",
    "with \n",
    "\n",
    "\\begin{equation}\n",
    "f(x) \\approx f(h) + hf'(x),\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "which we can rearrange to write an\n",
    "approximate expression for the derivative, $f'(x)$.\n",
    "\n",
    "\\begin{equation}\n",
    "f'(x) \\approx \\frac{f(x+h)-f(x)}{h}\n",
    "    \\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "The finite change, $h$, is called the step size, and\n",
    "equation (6) is known as the forward difference. Its geometric interpretation is\n",
    "described in Figure 1.<img src=\"./img/figure1.jpeg\" width = \"600\" alt=\"name1\" align=center />\n",
    "\n",
    "Although the finite difference method is useful and easy to implement,\n",
    "its accuracy can vary depending on the step size that is chosen. Suppose\n",
    "we wish to approximate the derivative of $f(x)=\\ln x$ using the forward\n",
    "difference method described in equation (6) using step\n",
    "sizes $h=\\{10^{-1},\\,\n",
    "10^{-7},\\,10^{-15}\\}$. This is rather unnecessary because the analytical\n",
    "derivative is just $f'(x) = 1/x$, but this example will serve to\n",
    "illustrate the drawbacks of the finite difference method. At\n",
    "$h=10^{-1}$, the numerical derivative is inaccurate because the step\n",
    "size is too large, making the calculations susceptible to truncation\n",
    "error. Conversely, at $h=10^{-15}$, the forward difference method also\n",
    "gives inaccurate results because the calculations can only be\n",
    "represented to a finite precision by the hardware in use. Hence,\n",
    "rounding error also affects the stability of the finite difference\n",
    "method.\n",
    "\n",
    "In order to evaluate the derivative via forward mode AD we first\n",
    "construct a computational graph which encodes the composition and\n",
    "dependence of sub-function evaluations. It is important to note that\n",
    "each elementary function evaluation needs to be understood at a symbolic\n",
    "level! After constructing the graph, the chain rule is simply applied\n",
    "successively to evaluations at a single point which then generates a\n",
    "table known as the computational trace. To illustrate the concept,\n",
    "consider the following example, adapted from *Evaluating derivatives:\n",
    "principles and techniques of algorithmic differentiation* by Griewank\n",
    "and Walther.\n",
    "\\begin{equation}\n",
    "f(x,y) = \\left[\\sin\\left(\\frac{x}{y}\\right) + \\frac{x}{y} - \\exp(y)\\right]\n",
    "    \\left[\\frac{x}{y} - \\exp(y)\\right]\n",
    "    \\tag{7}\n",
    "\\end{equation}\n",
    "A function of two arguments (i.e. variables $x$ and\n",
    "$y$) like the one in equation (7) can be\n",
    "evaluated at point by replacing the arguments with numerical values. The\n",
    "series of calculations needed to carry out the evaluation can be\n",
    "visualized as a computation graph, as shown in Figure 2.<img src=\"./img/figure2.png\" width = \"800\" alt=\"name\" align=center />\n",
    "The graph helps visualize the order of the\n",
    "computations, and it also establishes the dependence of successive\n",
    "calculations on previous ones.\\\n",
    "Although in many cases, the AD approach is given the entire function as\n",
    "input, this is not always the case. This necessitates the need for a\n",
    "\\\"seed\\\" value. In the basic case when the entire function is given, we\n",
    "can simply initialize the computational trace (discussed below) as 1.\n",
    "However, given the need for modularity which respects the chain rule, it\n",
    "is also possible to input a different seed, allowing for the integration\n",
    "of AD onto a sub segment of the function.\n",
    "\n",
    "\\\n",
    "The procedure for constructing such a graph is the following. Define a\n",
    "node for each of the inputs using a new variable. For this example, we\n",
    "will let $x_1 = x$ and $x_2 = y$. From there, any successive nodes may\n",
    "accept two inputs at maximum, and each node represents a new\n",
    "calculation. For example, in the computational graph shown in Figure 3, the node $x_3$ represents the calculation\n",
    "$x_1/x_2$, which later becomes an input in successive nodes. While the\n",
    "computational graph is useful for visualizing the entire computation\n",
    "procedure, the computational trace is useful for storing values as the\n",
    "computation is carried out. The computational trace for\n",
    "$f(1.5000, 0.5000)$ is given in Table 1![Table1](img/table1.png) In reference to the above, we can note that in\n",
    "this trace, the seed is 1 and thus suppressed as is often notated. In\n",
    "addition, it is important to note that depending on what implementation\n",
    "of forward mode the exact target computed can be slightly different. In\n",
    "particular, the vectorized extension of our milestone 2 code computes\n",
    "the Jacobian.\n",
    "\n",
    "Beyond the basic forward mode, there exists other implementations of AD\n",
    "in the so called \"reverse mode\\\". This approach can increase efficiency\n",
    "significantly in cases when the number of inputs is far greater than the\n",
    "number of outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 How to use\n",
    "\n",
    "## 3.1 <span id=\"jump\">Installation</span>\n",
    "\n",
    "The `github` url to the project is https://github.com/Software-Samurais/cs207-FinalProject.\n",
    "####  - Download the package from GitHub to your folder via these commands (in the terminal)\n",
    "Assuming that the user already has the latest version of `Python` 3, `Git` and a package manager of choice installed (like `pip`).\n",
    "\n",
    "\n",
    "\n",
    "```shell\n",
    "# Clone the repo\n",
    "git clone https://github.com/Software-Samurais/cs207-FinalProject.git\n",
    "\n",
    "```\n",
    "\n",
    "#### - Create a virtual environment and activate it (for Mac and Ubutun)\n",
    "\n",
    "```shell\n",
    "# If you don't have virtualenv, install it\n",
    "sudo easy_install virtualenv\n",
    "# Create virtual environment\n",
    "virtualenv env\n",
    "# Activate your virtual environment\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "#### -  Install the necessary dependencies :\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "#### - Run module tests (in the root directory)\n",
    "```shell\n",
    "# Run module tests if you like\n",
    "python -m pytest ./test/test_forward.py\n",
    "```\n",
    "\n",
    "## 3.2 Basic Demo\n",
    "\n",
    "In this part, we will give a basic demo for a quick guide to the user. \n",
    "\n",
    "Once AutoDiff is installed, the user must import it to be able to use it. The user will have the option to either import the entire library or to choose only a subset of modules, classes, or methods to import.\n",
    "\n",
    "In this demo, we import the package from autodiff directory and give an alias AD to the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package from parent directory \n",
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "import autodiff.AD as AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Denote constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Function value: 3.0, Derivative value: 1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constant (scalar or vector): User do not need to initialize derivative, it will set to 1.0\n",
    "x1 = AD.AutoDiff(3.0)\n",
    "# Each object has two attributes: val and der \n",
    "print(x1.val, x1.der)\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Denote scalar variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function value: 2.5, Derivative value: 2.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User can pass value and derivative of a variable\n",
    "x2 = AD.AutoDiff(2.5, 2.0)\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary Operations\n",
    "\n",
    "We can perform basic elementary operations on `AutoDiff` objects, just as we would with numbers. For example, addition, subtraction, multiplication, division, and power operations are all supported. Simply define an instance of the `AutoDiff` class to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = AD.AutoDiff(3.0)\n",
    "y = AD.AutoDiff(2.0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: -3.0, Derivative value: -1.0\n",
      "Function value: -2.0, Derivative value: -0.5\n"
     ]
    }
   ],
   "source": [
    "# Negative\n",
    "print(-x)\n",
    "print(-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 6.0, Derivative value: 1.0\n",
      "Function value: 5.0, Derivative value: 1.5\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "print(x + 3.)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 2.0, Derivative value: 1.0\n",
      "Function value: 1.0, Derivative value: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Subtraction\n",
    "print(x - 1.)\n",
    "print(x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 9.0, Derivative value: 3.0\n",
      "Function value: 6.0, Derivative value: 3.5\n"
     ]
    }
   ],
   "source": [
    "# Multiplication\n",
    "print(3.* x)\n",
    "print(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 1.0, Derivative value: 0.3333333333333333\n",
      "Function value: 0.3333333333333333, Derivative value: -0.1111111111111111\n",
      "Function value: 1.5, Derivative value: 0.125\n"
     ]
    }
   ],
   "source": [
    "# Division\n",
    "print(x / 3.)\n",
    "print(1 / x)\n",
    "print(x / y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 9.0, Derivative value: 6.0\n",
      "Function value: 8.0, Derivative value: 6.0\n"
     ]
    }
   ],
   "source": [
    "# Power\n",
    "print(x ** 2)\n",
    "print(y ** 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigonometric Functions, Exponentials, and Logarithms\n",
    "\n",
    "Other basic elementary operations, such as trigonometric functions, exponentials, and natural logarithms are also supported. Example uses of each of these are shown in the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 0.1411200080598672, Derivative value: -0.9899924966004454\n",
      "Function value: -0.27941549819892586, Derivative value: 3.360596003276281\n"
     ]
    }
   ],
   "source": [
    "# Sine\n",
    "print(AD.sin(x))\n",
    "print(AD.sin(x*y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: -0.9899924966004454, Derivative value: -0.1411200080598672\n",
      "Function value: 0.5403023058681398, Derivative value: -0.42073549240394825\n"
     ]
    }
   ],
   "source": [
    "# Cosine\n",
    "print(AD.cos(x))\n",
    "print(AD.cos(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: -0.1425465430742778, Derivative value: 1.020319516942427\n",
      "Function value: 14.10141994717172, Derivative value: 24.98125556581156\n"
     ]
    }
   ],
   "source": [
    "# Tangent\n",
    "print(AD.tan(x))\n",
    "print(AD.tan(x/y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 20.085536923187668, Derivative value: 20.085536923187668\n",
      "Function value: 403.42879349273517, Derivative value: 605.1431902391028\n"
     ]
    }
   ],
   "source": [
    "# Exponential\n",
    "print(AD.exp(x))\n",
    "print(AD.exp(y)**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value: 1.0986122886681098, Derivative value: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Logarithm\n",
    "print(AD.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Functions\n",
    "\n",
    "Customized functions can be defined using these elementary operations. For example, suppose we wish to work with the function\n",
    "\\begin{equation}\n",
    "    f(x) = \\exp\\left(\\sin x^2\\right) - x^4 + \\log(x). \n",
    "\\end{equation}\n",
    "We may define $f(x)$ using the standard Python conventions. However, instead of using the `exp` and `sin` methods from Numpy, we must use the `exp` and `sin` methods defined in the `AD` module because only the latter accept instances of the `AutoDiff` class as inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function value: -78.39137437130643, Derivative value: -115.9215797663472"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return AD.exp(AD.sin(x**2)) - x**4 + AD.log(x)\n",
    "\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Application: Newton's Method\n",
    "\n",
    "Given an equation, $f(x)$, Newton's method allows us to find solutions to $f(x)=0$ iteratively. Starting with an initial guess, $x_0$, we evaluate the function and its derivative at $x_0$. If $|f(x_0)| < \\epsilon$, where $\\epsilon$ is some tolerance value much less than one, then the iteration stops. Otherwise, the value of the next guess, $x_1$, is obtained as follows.\n",
    "\\begin{equation}\n",
    "    x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}\n",
    "\\end{equation}\n",
    "Following the same scheme, the $n$-th iteration can be written as \n",
    "\\begin{equation}\n",
    "    x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})},\n",
    "\\end{equation}\n",
    "where $x_{n-1}$ is the guess at the previous iteration.\n",
    "\n",
    "Since Newton's method makes use of the derivative of the function of interest, we may use automatic differentiation to implement Newton's method! Traditionally, we might consider using the finite difference method to approximate $f'(x)$ or define it explicitly in our routine. There are drawbacks to each of these approaches. The finite difference method is easy to implement, but its accuracy is strongly dependent on choosing the right step size. If $f(x)$ is a simple function, then explicitly defining $f'(x)$ is not a problem. However, this is not always possible. With the `AD` module, there is no need to approximate or explicitly define $f'(x)$. As we operate on our initial guess, $x_0$, the value of the derivative is automatically calculated as well, making it easy to implement Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, x0, tol=1e-16, max_iter=100):\n",
    "    \"\"\"Solves f(x) = 0 using Newton's method.\n",
    "    \n",
    "    Args:\n",
    "    =========\n",
    "    f (function): Function of interest\n",
    "    x0 (float): Initial guess\n",
    "    tol (float): Tolerance value\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    =========\n",
    "    xn.val (float): Solution to f(x) = 0 if it exists\n",
    "                    None if xn.der is zero or if the maximum number of \n",
    "                    iterations is reached without satisfying the stopping  \n",
    "                    criteria\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial guess\n",
    "    xn = x0\n",
    "    \n",
    "    for n in range(max_iter):\n",
    "        \n",
    "        # Calculate f(xn) and f'(xn) using the AutoDiff class\n",
    "        fn = f(xn)\n",
    "        \n",
    "        # Stop iterating if |f(xn)| is less than the tolerance value and return \n",
    "        # the solution, xn\n",
    "        if abs(fn.val) < tol:\n",
    "            print(f\"Found a solution after {n} iterations.\")\n",
    "            return xn.val\n",
    "        \n",
    "        # Check if the derivative is zero\n",
    "        if fn.der == 0:\n",
    "            raise ValueError(\"Encountered zero derivative. No solution.\")\n",
    "            \n",
    "        # Update guess\n",
    "        xn = xn - fn.val/fn.der\n",
    "        \n",
    "    # Stop iterating if no solution is found within the allowed number of \n",
    "    # iterations\n",
    "    print(\"Exceeded maximum number of iterations.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a solution after 6 iterations.\n",
      "Solution: 1.618033988749895\n"
     ]
    }
   ],
   "source": [
    "# Function of interest\n",
    "def f(x):\n",
    "    return x**2-x-1\n",
    "\n",
    "# Initial guess   \n",
    "x0 = AD.AutoDiff(1.0)\n",
    "\n",
    "print(f\"Solution: {newton(f, x0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Software Organization\n",
    "\n",
    "## 4.1 Directory Structure\n",
    "The package's directory will be structured as follows:\n",
    "``` py\n",
    "Ccs207-FinalProject\n",
    "\t__init__.py                                         #Initialization\n",
    "    /autodiff                                           #Back-end source code\n",
    "\t    /config                                         #Configuration for the project\n",
    "\t    AD.py\n",
    "    /gui                                                #Front-end source code\n",
    "        /dist                                           #Static css, js etc.\n",
    "        /template                                       #Web html files\n",
    "        /img                                            #Images used for font-end\n",
    "    /utils                                              #Preprocessing scripts\n",
    "\t/test                                               #Test cases\n",
    "\t\ttest_vector.py\n",
    "\t\ttest_forward.py\n",
    "        test_reverse.py\n",
    "\t/docs                                               #Documentation and records\n",
    "\t\tmilestone1.pdf\n",
    "        milestone2.ipynb\n",
    "\t/demo\n",
    "    requirements.txt                                    #Packages on which the program depends\n",
    "    README.md                                           #Introduction for the project\n",
    "```\n",
    "##  4.2 Basic Modules and functionality\n",
    "\n",
    "So far, we have simple forward mode for auto-differentiation in `AD` module.\n",
    "\n",
    "- `AD`: This module contains our custom library for autodifferentiation. \n",
    "    - It includes functionality for a AutoDiff class that contains values and derivatives. In the class, we override the operator like `__repr__`, `__neg__`, `__add__`, `__radd__`, `__sub__`, `__rsub__`, `__mul__`, `__rmul__`, `__truediv__`, `__rtruediv__`, `__pow__`.\n",
    "    - In addition, we define class-specific functions e.g., `sine`, `cosine`, `tangent`, `exponentiation`, `logarithm`. Thus the user could use our defined math function easily (as we use numpy).\n",
    "\n",
    "We plan to include reverse mode and other modules for application in the later work:\n",
    "\n",
    "- `optimize`: This module is designed to perform optimization. Users can define a custom function to optimize. For example, the first n−1 columns denote the features of the data, and the final column represents the labels. The user could specify the function to optimize as “mse”. Then, the function will find a local minimum of the mean squared error objective function. Finally, the module allows for static and animated plots for visualization.\n",
    "\n",
    "- `rootfinding`: This module is designed to find roots of a given function. It includes Newton’s method. It also allows the user to visualize static or animated results for visualization.\n",
    "\n",
    "    \n",
    "## 4.3 Test Suite\n",
    "Coding is the fundamental part of software development. Equally significant is build and testing. \n",
    "- We would utilize Travis CI and CodeCov to make the development process more reliable and professional. \n",
    "\n",
    "     `Travis CI` is used as a distributed CI (Continuous Integration) tools to build and automate test the project.\n",
    "     \n",
    "     `CodeCov` is used for test results analysis (eg. measuring test code coverage) and visu- alization.\n",
    "\n",
    "We have already set up these integrations, with badges included in the README.md, showing that build passed 100%, and coverage 100% .\n",
    "- Users can run the test suite by running in the root folder:\n",
    "```shell\n",
    "python -m pytest ./test/test_forward.py\n",
    "```\n",
    "- All test files will be placed in the test folder.\n",
    "\n",
    "    `test_forward`: It includes tests for scalar functions to ensure that the AD module properly calculates values of scalar functions and gradients with respect to scalar inputs.\n",
    "    \n",
    "    we plan to add more test files in future work:\n",
    "\n",
    "    `test_rootfinding`: This is a test suite for rootfinding.\n",
    "    \n",
    "    `test_optimize`: This is a test suite for optimization.\n",
    "\n",
    "\n",
    "\n",
    "## 4.4 Software Package and Distribution:\n",
    "### Package distribution\n",
    "We will package our software using `PyPI` (Python Package Index) for release. Write and run ’setup.py’ to package the software and upload it to the distribution server, thus people in community could easily download our package by ’pip install’.\n",
    "### Version Control\n",
    "We will take Version Control into consideration according to the standard in Python Enhancement Proposal (PEP) 386. With version control, we can tell the user what changes we made and set clear boundaries for where those changes occurred.\n",
    "### Framework\n",
    "- For web development, we would use `Flask`, a micro web framework, which is suitable for a small team to complete the implementation of a feature-rich small website and easily add customized functions.\n",
    "- For GUI (Graphical User Interface), we may choose `Vue.js`, a JavaScript frame- work for building user interfaces and single-page applications. Because it offers many API (Application Program Interface) to integrate with existing projects and is easy to get started. It is better in code reuse compared to frameworks like `jQuery`.\n",
    "\n",
    "## 4.5 How to install the package\n",
    "\n",
    "At this point, our package isn't on `PyPI` (will distribute later). User can download and install our package manually as mentioned in [3.1 Installation](#jump).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5 Implementation\n",
    "\n",
    "## 5.1 Forward Mode\n",
    "\n",
    "### Core Data Structures\n",
    "\n",
    "In current implementation, we define our own class `AutoDiff` as the main data structure. For future work, we would use \n",
    "- **List and Numpy array** - for vectorization. Track how many independent variables have been created so far and store copies of these independent variables.\n",
    "\n",
    "- **Graph** - for reverse mode. Every leaf variables (aside from constants) is an instance of Node. A computational graph consists of multiple nodes. Each node has two main attributes, self.inputs and self.op which indicate inputs of the current node and the operation between inputs. The input list is a python list whose elements are inputs of the current node. For example, if `node_3 =  ad.Op_1(node_2, node_1)` , we have `node_3.inputs = [node_2, node_1]` and `node_3.op = ad.Op_1`. Computation between leaf variables will be reloaded and return a new node. By doing this, we can parse the input equation into a computational graph automatically.\n",
    "\n",
    "### Core Classes\n",
    "- `AutoDiff`: The core class for the scalar implementation of forward mode automatic differentiation is the `AutoDiff` class. Defining an instance of the class allows the user to store and update function and derivative values. When an instance of the `AutoDiff` class is defined, the user is able to easily store,and update function and derivative values. These function and derivative values are attributes of the `AutoDiff` class.\n",
    "\n",
    "#### Important Attributes\n",
    "**Note:** The following two attributes are considered private. The user should not access these directly.\n",
    "- `_val`: Stores the function value\n",
    "- `_der`: Stores the derivative value; defaults to `1.0` if the user does not pass in a second argument to `AutoDiff` when defining a new instance\n",
    "\n",
    "There exist getter and setter methods to access current values and set new values for `_val` and `_der`.\n",
    "\n",
    "#### Methods\n",
    "We have implemented in this milestone the following methods for the `AutoDiff` class:\n",
    "\n",
    "- `__init__`:initialize a AutoDiff class object, regardless of the user input, with values and derivatives stored as float.\n",
    "- `__repr__`: overload the print format, prints self in the form of Function value: [val] Derivative value: [der]) \n",
    "- `__neg__`: overload negitive function\n",
    "- `__add__`: overload add function to handle addition of AutoDiff class objects and addition of AutoDiff and non-AutoDiff objects\n",
    "- `__radd__`: preserve addition commutative property.\n",
    "- `__sub__`: overload sub function to handle addition of AutoDiff class objects and addition of AutoDiff and non-AutoDiff objects\n",
    "- `__rsub__`: preserve substraction commutative property.\n",
    "- `__mul__`: overload multiplication function to handle addition of AutoDiff class objects and addition of AutoDiff and non-AutoDiff objects\n",
    "- `__rmul__`: preserve multiplication commutative property.\n",
    "- `__truediv__`: overload division function to handle addition of AutoDiff class objects and addition of AutoDiff and non-AutoDiff objects\n",
    "- `__rtruediv__`: preserve division commutative property. \n",
    "-` __pow__`: extend power functions to AutoDiff class objects, power times should be a integer.\n",
    "\n",
    "\n",
    "#### Functions\n",
    "\n",
    "- `sin(x)`\n",
    "- `cos(x)`\n",
    "- `tan(x)`\n",
    "- `exp(x)`\n",
    "- `log(x)`\n",
    "\n",
    "The funtions above take as input a AutoDiff class object and return a new AutoDiff class object with its value and derivative updated according to the elementary function it corresponds to using the chain rule.\n",
    "\n",
    "### External Dependencies\n",
    "- Numpy: is used for mathematical calculations. \n",
    "\n",
    "- pytest: it provides us a systematic way to test \n",
    "\n",
    "- TravisCI and Codecov: they are our test suites.\n",
    "\n",
    "### Elementary Functions\n",
    "The following elementary functions are supported: sine, cosine, tangent, exponential, and logarithm. Recall from elementary calculus that the derivatives of these functions are the following.\n",
    "\\begin{align}\n",
    "    \\frac{d}{dx} \\sin x &= \\cos x\\\\\n",
    "    \\frac{d}{dx} \\cos x &= -\\sin x\\\\\n",
    "    \\frac{d}{dx} \\tan x &= \\sec^2 x = \\frac{1}{\\cos^2 x}, \\quad x \\neq \\frac{\\pi}{2}\\\\\n",
    "    \\frac{d}{dx} \\exp x &= \\exp x\\\\\n",
    "    \\frac{d}{dx} \\ln x &= \\frac{1}{x}, \\quad x > 0\n",
    "\\end{align}\n",
    "Note that $\\tan x$ and $\\ln x$ will raise a `ValueError` if the user passes in a function value that is outside the valid domain.\n",
    "\n",
    "The methods that execute these elementary functions are defined in the `AD` module and work as follows. Each method takes in a single argument that is an instance of the `AutoDiff` class and returns a new instance of the `AutoDiff` class with the appropriate function and derivative values. To avoid rounding error, the `check_tol` method in the `AD` module compares the calculated values with their rounded counterparts. For example, suppose we wish to calculate $\\tan x$ and its derivative at $x=\\pi/4$. The `check_tol` method ensures that `AD.tan(AD.AutoDiff(np.pi/4))` returns `Function value: 1.0, Derivative value: 2.0` rather than `Function value: 1.0, Derivative value: 1.999999...`.\n",
    "\n",
    "## 5.2 Planning work\n",
    "\n",
    "### Not implemented - Vectorizaton\n",
    "In this current release, our automatic differentiation package can only handle scalar to scalar functions. In the future, we will extend this work to include the more general category of scalar to scalar functions. This extension will primarily preserve the structure of the code but will result in rewriting the main functions to be able to deal with vector inputs.\n",
    "\n",
    "We are going to support gradient computation for vectors and matrixes.   For simplicity, we only discuss how to compute the gradient of the matrix because the vector is only the special case of a matrix.  Operations between matrixes have two classes. The first one is the element-wise operation. For this kind of operation, we do not have to change the code for scalar and only need to replace scalar values with `np.array`. The second one is the matrix-wise operation like matrix multiplication. For this kind of operation, we use `np.array` to represent the value of variables again. However, we need to reimplement the operations because they have different formulations of the gradient. For example, if the operation is matrix multiplication between matrix A and B, the returned gradient from output to A and B are $dA = dY \\times B^T$ and $dB = A^T \\times dY$.\n",
    "Reverse mode\n",
    "\n",
    "\n",
    "### Plan on implementing - Reverse mode, Optimization etc.\n",
    "More details could be found at [6 Future Features](#6).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 <span id='6'>Future Features </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some proposals for future features in the AD package. Some of these will be implemented in the final version!\n",
    "\n",
    "### Reverse Mode\n",
    "#### Overall Framework\n",
    "For the reverse mode, we need to build the computational graph and figure out the sequence of computation correctly to get the right gradient.\n",
    "#### Ststic V.S. Dynamic Graph\n",
    "Basically, there are two strategies of building the computational graph, static graph and dynamic graph. The former computes value and gradient after finishing graph and the latter do the computation dynamically. Although dynamic is easier to debug and more user-friendly, static graph is more clear to understand reverse mode and more suitable for this course. So we choose static graph based reverse mode as our main framework.\n",
    "#### Attributes and Data Structure Used in Class Node\n",
    "A computational graph consists of multiple nodes. Each node has two main attributes, self.inputs and self.op which indicate inputs of current node and the operation between inputs. It is worth mentioning that nodes are created by their own operation. The input list is a python list whose elements are inputs of the current node. For example, if `node_3 =  ad.Op_1(node_2, node_1)` , we have `node_3.inputs = [node_2, node_1]` and `node_3.op = ad.Op_1`. Several operations are implemented, such as add, add_by_const, mul, mul_by_const, tan, sin and so on. \n",
    "#### How to Support Higher-Order Gradient\n",
    "To better support higher order gradient computation, gradients of leaf and intermediate variable are also represented by nodes. As a result, our computational graph not only has nodes representing values of variable during forward propagation, but also has nodes representing gradients of variable during back propagation. If users want to compute higher order gradient, they can easily add new nodes representing gradient of nodes of low order gradient. With this mechanism, we can compute any order gradient.\n",
    "#### Traveral on Graph\n",
    "How to find the path of creating nodes of gradients in the graph is also quite important to get correct results efficiently. Given a list of nodes, we use a post-order Depth First Search (DFS) to get a topological sort list of nodes ending in our target nodes. The reverse of this list is the right sequence to add nodes of gradients.\n",
    "\n",
    "#### Efficiency of Our Implementation\n",
    "As we mentioned above, we use nodes to represent both values and gradients of variables in the same graph. This implementation has two advantages. First, to compute a gradient node, we may have to access to some associated value nodes. With our implementation, we do not have to recompute the results of these value nodes. Second, we do post-order DFS on the computational graph and try to find a topology-sorted list of nodes which means that we only need to use part of the graph to compute the result of a certain node we care about instead of executing the whole graph.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "\n",
    "\n",
    "#### What to realize\n",
    "Gradient Descent is used to find the local minimum of a function f by taking locally optimum steps in the direction of steepest descent. A common application is in machine learning when a user desires to find optimal weights to minimize a loss function.\n",
    "\n",
    "\n",
    "BFGS, short for “Broyden–Fletcher–Goldfarb–Shanno algorithm”, seeks a stationary point of a function, i.e. where the gradient is zero. In quasi-Newton methods, the Hessian matrix of second derivatives is not computed. Instead, the Hessian matrix is approximated using updates specified by gradient evaluations (or approximate gradient evaluations).\n",
    "\n",
    "Here is a pseudocode of the implementation of BFGS.<img src=\"./img/op1.jpeg\" width = \"400\" alt=\"name\" align=center />\n",
    "\n",
    "\n",
    "#### Method\n",
    "`GradientDescent:` solve for a local minimum of a function f:ℝm⇒ℝ1. If f is a convex function, then the local\n",
    "minimum is a global minimum.\n",
    "\n",
    "Input:\n",
    "\n",
    "- f: In machine learning applications, this should be the cost/objective function. If f is a function of multiple scalars (i.e. ℝm⇒ℝ1), the arguments to f should be passed in as a numpy array. In this case, define f as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "def f(variables):\n",
    "    x, y, z = variables\n",
    "    return x ** 2 + y ** 2 + z ** 2 + np.sin(x)\n",
    "```\n",
    "\n",
    "- x: int, float, or AD objects (multivariate). It is the initial guess for a root of f. If f is a scalar to scalar function (i.e. ℝ1⇒ℝ1), and the initial guess for the root is 1, then a valid x is x = 1. If f is a function of multiple scalars, with initial guess for the root as (1, 2, 3), then a valid definition of x is a numpy array.\n",
    "\n",
    "- iters: int, optional, default=1000. The maximum number of iterations to run the Newton root finding algorithm. The algorithm will run for min (t,iters) iterations, where t is the number of steps until tol is satisfied.\n",
    "\n",
    "- tol: float, optional, default=1e-8. If the difference is smaller than tol, then the algorithm will terminate, even if the number of iterations has not reached iters.\n",
    "\n",
    "Output:\n",
    "\n",
    "- minimum: AD obeject ∈ℝm. The val attribute contains a numpy array of the minimum that the algorithm found in min(iters,t) iterations (iters,t defined above). The der attribute contains the Jacobian value at the specified root.\n",
    "- var_path: a numpy array (ℝn×m), where n=min(iters,t) is the number of steps of the algorithm and m if the dimension of the minimum, where rows of the array are steps taken in consecutive order.\n",
    "- g_path: a numpy array (ℝn×1), containing the consecutive steps of the output of f at each guess in var_path.\n",
    "\n",
    "`plot_result:` A plot function that can visulize the gradient descent process\n",
    "\n",
    "Input: \n",
    "- f: the function we want to optimize\n",
    "- var_pathh: a numpy array (ℝn×m), where n=min(iters,t) is the number of steps of the algorithm and m if the dimension of the minimum, where rows of the array are steps taken in consecutive order.\n",
    "- g_path: a numpy array (ℝn×1), containing the consecutive steps of the output of f at each guess in var_path.\n",
    "- f_string: string, title of the plot \n",
    "- x_lims, y_lims: two tuples, to set range of X-axes and Y-axes\n",
    "\n",
    "Output: None, just plot a figure.\n",
    "\n",
    "\n",
    "#### How to use\n",
    "For example, we want to find minimum of quartic function f(x)=x4. \n",
    "\n",
    "```python\n",
    "import AutoDiff.optimize as opt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "        return x ** 4 + 2 * (x ** 3) - 12 * (x ** 2) - 2 * x + 6\n",
    "\n",
    "# Function string to include in plot\n",
    "f_string = 'f(x) = x^4 + 2x^3 -12x^2 -2x + 6'\n",
    "\n",
    "x0 = 4\n",
    "solution, xy_path, f_path = opt.GradientDescent(f, x0, iters=1000, eta=0.002)\n",
    "# A plot function that can visulize the process\n",
    "opt.plot_results(f, xy_path, f_path, f_string, x_lims=(-6, 5), y_lims=(-100, 70))\n",
    "```\n",
    "\n",
    "#### Changes to current version\n",
    "\n",
    "- Directory structure: do not change a lot, just add files in autodiff directory\n",
    "- New modules: add optimize module\n",
    "- Classes: may add optimize class, and some method mentioned above\n",
    "- data structures: basically use numpy array, also list\n",
    "\n",
    "#### Challenges\n",
    "- It's may be difficult to plot a figure when the input is high dimension, we will try to figure it out refer to some existing packages\n",
    "- S"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}